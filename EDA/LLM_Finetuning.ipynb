{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interior-cartoon",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-rouge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /h/omidv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /h/omidv/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from time import sleep\n",
    "from typing import List, Callable\n",
    "import jiwer\n",
    "import openai\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import BERTScorer\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-morris",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "timely-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(reference, hypothesis):\n",
    "    return jiwer.wer(reference, hypothesis)\n",
    "\n",
    "def compute_bleu(references, hypotheses):\n",
    "    return corpus_bleu(hypotheses, references).score\n",
    "\n",
    "def compute_meteor(references, hypotheses):\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores.append(meteor_score([ref.split()], hyp.split()))\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "def compute_bertscore(references, hypotheses):\n",
    "    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    p, r, f1 = scorer.score(hypotheses, references)\n",
    "    bert_score = {\n",
    "            'precision': p.mean().item(),\n",
    "            'recall': r.mean().item(),\n",
    "            'f1': f1.mean().item()\n",
    "        }\n",
    "    return bert_score\n",
    "\n",
    "def construct_input(question):\n",
    "    prompt = [{\"role\": \"user\", \"content\": question}]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "matched-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset:Dataset, model: str, client: openai.OpenAI, postprocessing: Callable[[List[str]], str], generation_config: dict, \n",
    "                   use_llm: bool = True, verbose: int = 0, step: int = 100) -> dict:\n",
    "    \"\"\"Evaluate model performance on the entire dataset.\"\"\"\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    running_wer = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        # Get hypotheses for current example\n",
    "        hypotheses = [h.strip() for h in dataset['source'][i].split('.') if h.strip()]\n",
    "        reference = dataset['target'][i]\n",
    "        \n",
    "        # Generate prompt\n",
    "        if use_llm:\n",
    "            llm_prompt = postprocessing(hypotheses)\n",
    "            messages = construct_input(llm_prompt)\n",
    "\n",
    "            try:\n",
    "                generation = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    **generation_config\n",
    "                )\n",
    "                prediction = generation.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {i}: {e}\")\n",
    "                prediction = \"\"\n",
    "        else:\n",
    "            prediction = postprocessing(hypotheses, reference)\n",
    "        reference, prediction = reference.lower(), prediction.lower()\n",
    "        wer = jiwer.wer(reference, prediction)\n",
    "        running_wer.append(wer)\n",
    "        all_predictions.append(prediction)\n",
    "        all_references.append(reference)\n",
    "        \n",
    "        # Print progress update for every %step examples\n",
    "        if (i + 1) % step == 0:\n",
    "            print(f\"Current average WER: {round(np.mean(running_wer).item(), 3):.3f}\")\n",
    "            if verbose == 1:\n",
    "                print('-----------------------------------------------------------')\n",
    "                print(\"Corrected: %s\\nTarget:    %s\\n\"%(prediction, reference))\n",
    "                \n",
    "    # Calculate metrics\n",
    "    bertscore = compute_bertscore(all_predictions, all_references)\n",
    "    metrics = {\n",
    "        'WER': round(np.mean(running_wer).item(), 3),\n",
    "        'METEOR': round(compute_meteor(all_predictions, all_references), 3),\n",
    "        'BERT Precision': round(bertscore['precision'], 3),\n",
    "        'BERT Recall': round(bertscore['recall'], 3),\n",
    "        'BERT F1': round(bertscore['f1'], 3),\n",
    "        #'BLEU': round(compute_bleu(all_predictions, all_references), 3),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-replacement",
   "metadata": {},
   "source": [
    "### Specify Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beginning-attraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "model=\"Meta-Llama-3.1-8B-Instruct\"\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "small_generation_config = {\"max_tokens\": 20, \"temperature\": 0.9}\n",
    "moderate_generation_config = {\"max_tokens\": 200, \"temperature\": 0.9}\n",
    "\n",
    "# If model is not yet available, try again after some delay.\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Please introduce yourself.\"}],\n",
    "        )\n",
    "    \n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "universal-canvas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Djibouti is Djibouti City.\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_input(\"What is the capital of Djibouti?\")\n",
    "generation = client.chat.completions.create(model=model, messages = prompt, **small_generation_config)\n",
    "print(generation.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-master",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "combined-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines\n",
    "def get_oracle_hypothesis(hypotheses: List[str], reference: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the hypothesis that gives the lowest WER compared to the reference.\n",
    "    Returns the best hypothesis and its WER.\n",
    "    \"\"\"\n",
    "    wers = [jiwer.wer(reference, hyp) for hyp in hypotheses]\n",
    "    best_idx = np.argmin(wers)\n",
    "    return hypotheses[best_idx]\n",
    "\n",
    "def get_top1_hypothesis(hypotheses: List[str], reference: str) -> str:\n",
    "    \"\"\"Get the first hypothesis (baseline).\"\"\"\n",
    "    return hypotheses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "promising-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_unconstrained(hypotheses: List[str]) -> str:\n",
    "    prompt = \"Perform error correction on the top5 outputs generated by an Automatic Speech Recognition\"\n",
    "    \"(ASR) system. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\\n\\n\"\n",
    "    for idx, hypothesis in enumerate(hypotheses):\n",
    "        prompt += \"<hypothesis\"+ str(idx) + \">\" + hypothesis + \"</hypothesis\"+ str(idx) + \">\\n\"\n",
    "    return prompt + \"Please provide the corrected top1 ASR transcription of the given utterance only, do not add any explanation or other words.\"\n",
    "\n",
    "def zero_shot_constrained(hypotheses: List[str]) -> str:\n",
    "    prompt = \"Perform language model rescoring based on the top5 outputs generated by an Automatic Speech Recognition\"\n",
    "    \"(ASR) system. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\\n\\n\"\n",
    "    for idx, hypothesis in enumerate(hypotheses):\n",
    "        prompt += \"<option\"+ str(idx) + \">\" + hypothesis + \"</option\"+ str(idx) + \">\\n\"\n",
    "    return prompt + \"Please output the selected top1 ASR transcription, do not add any explanation or <option> tags.\"\n",
    "\n",
    "def zero_shot_closest(hypotheses: List[str]) -> str:\n",
    "    # TO DO\n",
    "    \n",
    "def zero_shot_lattice(hypotheses: List[str]) -> str:\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-locking",
   "metadata": {},
   "source": [
    "# Common Voice Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unusual-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'best_hypo'],\n",
      "    num_rows: 1098\n",
      "})\n",
      "['transit road surveyed by joseph ellicott was named for an important surveying instrument', 'transit wrote surveyed by joseph ellicott was named for an important surveying instrument', 'transit road surveyed by joseph ellikot was named for an important surveying instrument', 'transit road surveyed by joseph ellicott was named for an important surveying instrument', 'transit road surveyed by joseph ellicate was named for an important surveying instrument']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv(\"/fs01/home/omidv/data/test_cv.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = [h.strip() for h in dataset['source'][0].split('.') if h.strip()]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "engaged-appreciation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 100/1098 [00:57<10:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.189\n",
      "-----------------------------------------------------------\n",
      "Corrected: it is commonly used interchangeably with terms political islam or islamic fundamentalism\n",
      "Target:    it is commonly used interchangeably with the terms political islam or islamic fundamentalism\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 200/1098 [01:58<09:11,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.207\n",
      "-----------------------------------------------------------\n",
      "Corrected: in some cases the purpose of this operation needs to curb excessive vomiting\n",
      "Target:    in some cases the purpose of this operation is to correct excessive vomiting\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 300/1098 [03:01<08:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.202\n",
      "-----------------------------------------------------------\n",
      "Corrected: a woman sits in the snow with a young child on her lap\n",
      "Target:    a woman sits in the snow with a young child on her lap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 400/1098 [04:03<06:23,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.184\n",
      "-----------------------------------------------------------\n",
      "Corrected: she saw he was red and hurt and is fallen\n",
      "Target:    she saw he was rather pleased and her anxiety all went\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 500/1098 [05:04<05:43,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.179\n",
      "-----------------------------------------------------------\n",
      "Corrected: on television and radio the point has fronted sport travel and rural affairs programs\n",
      "Target:    on television and radio vipond has fronted sport travel and rural affairs programs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 600/1098 [06:05<04:37,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.179\n",
      "-----------------------------------------------------------\n",
      "Corrected: a major local employer is the wespac semis power plant\n",
      "Target:    a major local employer is the w h sammis power plant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 700/1098 [07:07<04:10,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.185\n",
      "-----------------------------------------------------------\n",
      "Corrected: these and other indicators reveal a seriously declining performance standard in the city is\n",
      "Target:    these and other indicators reveal a seriously declining performance standard in the city is schools\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 800/1098 [08:05<02:38,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.184\n",
      "-----------------------------------------------------------\n",
      "Corrected: several shows have also been announced for north america in december\n",
      "Target:    several shows have also been announced for north america in december\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 900/1098 [09:07<01:49,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.185\n",
      "-----------------------------------------------------------\n",
      "Corrected: there is a second of brooding loneliness in existence\n",
      "Target:    there is not a second of brooding loneliness in their existence\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1000/1098 [10:08<01:03,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.187\n",
      "-----------------------------------------------------------\n",
      "Corrected: his first movie role was in roger corman's streetwalker playing a pimp named duke\n",
      "Target:    his first movie role was in roger corman is streetwalkin playing a pimp named duke\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1098/1098 [11:07<00:00,  1.65it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = evaluate_model(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                 small_generation_config, use_llm=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arabic-sudan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 101/1098 [00:56<07:47,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.136\n",
      "-----------------------------------------------------------\n",
      "Corrected: it is commonly used interchangeably with terms political islam or islamic fundamentalism\n",
      "Target:    it is commonly used interchangeably with the terms political islam or islamic fundamentalism\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 200/1098 [01:55<08:40,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.159\n",
      "-----------------------------------------------------------\n",
      "Corrected: in some cases the purpose of this operation needs to curb excessive vomiting\n",
      "Target:    in some cases the purpose of this operation is to correct excessive vomiting\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 300/1098 [02:57<08:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.156\n",
      "-----------------------------------------------------------\n",
      "Corrected: a woman sits in the snow with a young child on her lap\n",
      "Target:    a woman sits in the snow with a young child on her lap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 400/1098 [03:59<06:24,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.145\n",
      "-----------------------------------------------------------\n",
      "Corrected: she saw he was red pleased and hurt and is fallen\n",
      "Target:    she saw he was rather pleased and her anxiety all went\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 500/1098 [04:59<05:37,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.142\n",
      "-----------------------------------------------------------\n",
      "Corrected: on television and radio the point has fronted sport travel and rural affairs programs\n",
      "Target:    on television and radio vipond has fronted sport travel and rural affairs programs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 600/1098 [05:59<04:33,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.145\n",
      "-----------------------------------------------------------\n",
      "Corrected: a major local employer is the who semis power plant\n",
      "Target:    a major local employer is the w h sammis power plant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 700/1098 [07:00<03:55,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.147\n",
      "-----------------------------------------------------------\n",
      "Corrected: these and how the indicators reveal a seriously declining performance standard in the city is\n",
      "Target:    these and other indicators reveal a seriously declining performance standard in the city is schools\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 800/1098 [07:59<02:40,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.149\n",
      "-----------------------------------------------------------\n",
      "Corrected: several shows have also been announced for north america in december\n",
      "Target:    several shows have also been announced for north america in december\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 900/1098 [09:00<01:54,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.149\n",
      "-----------------------------------------------------------\n",
      "Corrected: there is in a second of brooding loneliness in the existence\n",
      "Target:    there is not a second of brooding loneliness in their existence\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1000/1098 [10:00<01:02,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.151\n",
      "-----------------------------------------------------------\n",
      "Corrected: his first movie role was in roger corman street walking playing a pimp named duke\n",
      "Target:    his first movie role was in roger corman is streetwalkin playing a pimp named duke\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1098/1098 [11:00<00:00,  1.66it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_constrained = evaluate_model(dataset, model, client, zero_shot_constrained,\n",
    "                                                small_generation_config, use_llm=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "precious-control",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 154/1098 [00:00<00:03, 300.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 243/1098 [00:00<00:03, 266.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 336/1098 [00:01<00:02, 292.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 460/1098 [00:01<00:02, 299.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 553/1098 [00:01<00:01, 302.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 644/1098 [00:02<00:01, 297.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 735/1098 [00:02<00:01, 300.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 859/1098 [00:02<00:00, 300.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 952/1098 [00:03<00:00, 299.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1044/1098 [00:03<00:00, 300.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1098/1098 [00:03<00:00, 294.09it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 12%|█▏        | 133/1098 [00:00<00:03, 319.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 265/1098 [00:00<00:02, 323.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 366/1098 [00:01<00:02, 329.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 467/1098 [00:01<00:01, 331.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 535/1098 [00:01<00:01, 330.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 637/1098 [00:01<00:01, 331.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 737/1098 [00:02<00:01, 325.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 836/1098 [00:02<00:00, 322.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 936/1098 [00:02<00:00, 326.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1035/1098 [00:03<00:00, 326.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average WER: 0.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1098/1098 [00:03<00:00, 326.16it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metrics_get_oracle_hypothesis = evaluate_model(dataset, model, client, get_oracle_hypothesis,\n",
    "                                               small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = evaluate_model(dataset, model, client, get_top1_hypothesis,\n",
    "                                             small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "guilty-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER   BLEU  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.149  0.142   0.876           0.789        0.800    0.794\n",
      "Zero-shot Uncon   0.189  0.621   0.847           0.771        0.788    0.780\n",
      "Zero-shot Constr  0.152  0.621   0.872           0.786        0.799    0.793\n",
      "Oracle            0.112  0.142   0.903           0.828        0.840    0.834\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "print(df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-speech",
   "metadata": {},
   "source": [
    "# Wall Street Journal Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv(\"/fs01/home/omidv/data/test_wsj_score.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = [h.strip() for h in dataset['source'][0].split('.') if h.strip()]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_zero_shot_unconstrained = evaluate_model(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                 small_generation_config, use_llm=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_zero_shot_constrained = evaluate_model(dataset, model, client, zero_shot_constrained,\n",
    "                                                small_generation_config, use_llm=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_get_oracle_hypothesis = evaluate_model(dataset, model, client, get_oracle_hypothesis,\n",
    "                                               small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = evaluate_model(dataset, model, client, get_top1_hypothesis,\n",
    "                                             small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'BLEU', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-daughter",
   "metadata": {},
   "source": [
    "# SwitchBoard Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv(\"/fs01/home/omidv/data/test_swbd.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = [h.strip() for h in dataset['source'][0].split('.') if h.strip()]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_zero_shot_unconstrained = evaluate_model(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                 small_generation_config, use_llm=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_zero_shot_constrained = evaluate_model(dataset, model, client, zero_shot_constrained,\n",
    "                                                small_generation_config, use_llm=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_get_oracle_hypothesis = evaluate_model(dataset, model, client, get_oracle_hypothesis,\n",
    "                                               small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = evaluate_model(dataset, model, client, get_top1_hypothesis,\n",
    "                                             small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'BLEU', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-thread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-elder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-landscape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-extraction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-rates",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-truck",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-statistics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-plaza",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-oracle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-nancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-skill",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-ready",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-helping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-composite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-content",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-belfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "agreed-watson",
   "metadata": {},
   "source": [
    "# Few-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-message",
   "metadata": {},
   "source": [
    "We'll start by prompting the model to solve some word problems and build up to using the Few-Shot CoT method proposed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-ghana",
   "metadata": {},
   "source": [
    "First try \"zero-shot prompting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "worse-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = (\n",
    "    \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? just give final answer with no explanation.\"\n",
    ")\n",
    "zero_shot_prompt = construct_input(zero_shot_prompt)\n",
    "\n",
    "generation_example = client.chat.completions.create(model=model,messages = zero_shot_prompt, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-orbit",
   "metadata": {},
   "source": [
    "The correct answer is 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-adventure",
   "metadata": {},
   "source": [
    "Now let's try standard few-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "approximate-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: Benjamin is taking bottle inventory. He has two cases with 15 bottles in each and one with 7. How many bottles are there in total?\n",
      "A: The answer is 37.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The answer is ...\n",
      "Just give the final answer to the last question with no explanation.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: Benjamin is taking bottle inventory. He has two cases with \"\n",
    "    \"15 bottles in each and one with 7. How many bottles are there in total?\\nA: The answer is 37.\\n\\nQ: The \"\n",
    "    \"cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\nA: \"\n",
    "    \"The answer is ...\\nJust give the final answer to the last question with no explanation.\"\n",
    ")\n",
    "few_shot_message = construct_input(few_shot_prompt)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "vietnamese-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_message,  **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-recorder",
   "metadata": {},
   "source": [
    "Now, let's try prompting the model with a few-shot CoT prompt, where we provide an example of the kind of reasoning required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "appreciated-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. \"\n",
    "    \"5 + 6 = 11. The answer is 11.\\n\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 \"\n",
    "    \"more, how many apples do they have?\\nA:\"\n",
    ")\n",
    "\n",
    "few_shot_cot_prompt_message = construct_input(few_shot_cot_prompt)\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "coastal-hacker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the problem, follow these steps:\n",
      "\n",
      "1. Start with the initial number of apples: 23\n",
      "2. Subtract the number of apples used for lunch: 23 - 20 = 3\n",
      "3. Add the number of apples bought: 3 + 6 = 9\n",
      "\n",
      "The cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_cot_prompt_message, **moderate_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-empire",
   "metadata": {},
   "source": [
    "## An example from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-playing",
   "metadata": {},
   "source": [
    "Let's try to compare few-shot prompting with few-shot CoT for slightly different kind of problem. This example is drawn from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "young-parade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these\n",
      "A: \n",
      " what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of \"\n",
    "    \"the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these\\nA: \\n what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.\"\n",
    ")\n",
    "few_shot_prompt_message = construct_input(few_shot_prompt)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "voluntary-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(c)\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_prompt_message, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-mechanics",
   "metadata": {},
   "source": [
    "The correct choice for this problem is \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "human-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these \n",
      " what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers \"\n",
    "    \"is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: If 10 is added to each number, then the mean of the \"\n",
    "    \"numbers also increases by 10. So the new mean would be 50. The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these \\n what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.:\"\n",
    ")\n",
    "few_shot_cot_prompt_message = construct_input(few_shot_cot_prompt)\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "following-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_cot_prompt_message, **moderate_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-compilation",
   "metadata": {},
   "source": [
    "Sometimes the examples are not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-mongolia",
   "metadata": {},
   "source": [
    "# Zero-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-commonwealth",
   "metadata": {},
   "source": [
    "It can be tedious and tricky to form useful and effective reasoning examples. Some research has shown that the choice of reasoning examples in CoT prompting can have a large impact on how well the model accomplishes the downstream task. So let's try a zero-shot CoT approach devised in \"Large Language Models are Zero-Shot Reasoners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "disabled-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would be in each group?\n",
      "A: \n",
      "Just give the final answer to the last question with no explanation.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: There are 64 students trying out for the school's trivia \"\n",
    "    \"teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would \"\n",
    "    \"be in each group?\\nA: \\nJust give the final answer to the last question with no explanation.\"\n",
    ")\n",
    "\n",
    "\n",
    "few_shot_prompt_message = construct_input(few_shot_prompt)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "martial-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_prompt_message, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-hebrew",
   "metadata": {},
   "source": [
    "The correct answer to this problem is 7.\n",
    "\n",
    "Could you get the correct answer with this example?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-nation",
   "metadata": {},
   "source": [
    "# TASK: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-division",
   "metadata": {},
   "source": [
    "Try to do CoT without adding examples.\n",
    "\n",
    "\n",
    "Split into two stages:\n",
    "\n",
    "1) Reasoning Generation   \n",
    "2) Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "annual-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams.If 36 of them didn't get picked for the team and the rest were put into 4 groups, how manystudents would be in each group?\n",
      "A: \n",
      "Let's think step by step.\n"
     ]
    }
   ],
   "source": [
    "question_prompt = (\"Q: There are 64 students trying out for the school's trivia teams.\"\n",
    "                   \"If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many\"\n",
    "                   \"students would be in each group?\\nA: \\nLet's think step by step.\" )\n",
    "\n",
    "question_prompt_message = construct_input(question_prompt)\n",
    "print(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "occupied-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Determine the number of students who were picked for the team. There are 64 students total, and 36 didn’t get picked, which means that 64 - 36 = 28 students were picked for the team.\n",
      "\n",
      "Step 2: Divide the number of students who were picked for the team by the number of groups they were put into to determine the number of students in each group. Since there are 4 groups, we need to divide 28 by 4.\n",
      "\n",
      "28 ÷ 4 = 7\n",
      "\n",
      "The final answer is: $\\boxed{7}$\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model, messages = question_prompt_message, **moderate_generation_config)\n",
    "reasoning_extraction = generation_example.choices[0].message.content\n",
    "print(reasoning_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-tunisia",
   "metadata": {},
   "source": [
    "Try to get the correct answer (7) with no example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "featured-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: break down the problem into two steps. First, ask the model for reasoning,\n",
    "# then, given the reasoning, ask for the final answer by appending \"\\nTherefore, the answer is\" followed by the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "scientific-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams.If 36 of them didn't get picked for the team and the rest were put into 4 groups, how manystudents would be in each group?\n",
      "A: Let's think step by step.\n",
      "\n",
      "Step 1: Determine the number of students who were picked for the team. There are 64 students total, and 36 didn’t get picked, which means that 64 - 36 = 28 students were picked for the team.\n",
      "\n",
      "Step 2: Divide the number of students who were picked for the team by the number of groups they were put into to determine the number of students in each group. Since there are 4 groups, we need to divide 28 by 4.\n",
      "\n",
      "28 ÷ 4 = 7\n",
      "\n",
      "The final answer is: $\\boxed{7}$\n",
      "\n",
      "Therefore, what is the final answer in numerals? Don't say a single word except the final answer.\n"
     ]
    }
   ],
   "source": [
    "reasoning_prompt = (\"Q: There are 64 students trying out for the school's trivia teams.\"\n",
    "                   \"If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many\"\n",
    "                   \"students would be in each group?\\nA: Let's think step by step.\\n\\n\" + reasoning_extraction +\n",
    "                   \"\\n\\nTherefore, what is the final answer in numerals? Don't say a single word except the final answer.\")\n",
    "\n",
    "reasoning_prompt_message = construct_input(reasoning_prompt)\n",
    "print(reasoning_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "functioning-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model, messages = reasoning_prompt_message, **moderate_generation_config)\n",
    "reasoning_generation = generation_example.choices[0].message.content\n",
    "print(reasoning_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-honduras",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-tuesday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-benjamin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kscope",
   "language": "python",
   "name": "kscope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
