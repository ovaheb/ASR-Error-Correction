{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedicated-charm",
   "metadata": {},
   "source": [
    "# One-shot setting (uncon and closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "import logging\n",
    "import openai\n",
    "from metrics import MetricsCalculator\n",
    "from data_handler import DataHandler\n",
    "from llm_client import LLMClient\n",
    "from correction_strategies import (\n",
    "    OneShotUnconstrainedCorrection,\n",
    "    OneShotClosestCorrection,\n",
    "    OracleHypothesisSelection,\n",
    "    Top1HypothesisSelection\n",
    ")\n",
    "from evaluation_pipeline import EvaluationPipeline\n",
    "from utils import ProgressTracker\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "nest_asyncio.apply()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-turkish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominican-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env file created successfully\n"
     ]
    }
   ],
   "source": [
    "### For vector models\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(\"export OPENAI_BASE_URL=https://kscope.vectorinstitute.ai/v1\\n\")\n",
    "    #f.write(\"export OPENAI_API_KEY="")\n",
    "            \n",
    "print(\".env file created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabulous-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env file created successfully\n",
      " Key: ""
     ]
    }
   ],
   "source": [
    "#### For Open ai\n",
    "with open(\".env\", \"w\") as f:\n",
    "    \n",
    "    #f.write(\"\"\" export KEY="")\n",
    "print(\".env file created successfully\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "print(\" Key:\", os.getenv(\"KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-gateway",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an AI language model created by OpenAI, designed to assist with a wide range of queries by generating human-like text based on the input I receive. I can help with information, advice, creative writing, and more. If you have any questions or need assistance, feel free to ask!\n",
      "Dataset({\n",
      "    features: ['input', 'output', 'input1', 'input2'],\n",
      "    num_rows: 2259\n",
      "})\n",
      "Evaluating One-shot Unconstrained:\n",
      "Submitted all tasks!\n",
      "Progress: 331/2259 tasks completed\r"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "metrics_calculator = MetricsCalculator()\n",
    "data_handler = DataHandler()\n",
    "progress_tracker = ProgressTracker()\n",
    "llm_client = LLMClient(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "evaluation_pipeline = EvaluationPipeline(metrics_calculator, data_handler, progress_tracker)\n",
    "\n",
    "model = \"gpt-4o\" # Or \"gpt-3.5-turbo\"  \"gpt-4o\"  \"gpt-4o-mini\"\n",
    "small_generation_config = {\"max_tokens\": 30, \"temperature\": 1}\n",
    "large_generation_config = {\"max_tokens\": 100, \"temperature\": 0.9}\n",
    "\n",
    "# Model availability check (optional - you can run this cell to check)\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = await llm_client.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Please introduce yourself.\"}],\n",
    "        )\n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "if output:\n",
    "    print(output.choices[0].message.content)\n",
    "\n",
    "# Load Dataset (run this cell)\n",
    "df = pd.read_json(\"~/projects/ASR-Error-Correction/data/test_lrs2.json\")# Adjust path if needed\n",
    "results_path = \"~/projects/ASR-Error-Correction/codes/one_shot_setting/results/corrected_test_lrs2.json\" # Adjust path if needed\n",
    "os.makedirs(os.path.expanduser(\"~/projects/ASR-Error-Correction/codes/one_shot_setting/results\"), exist_ok=True)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "\n",
    "# Run Evaluation (run this cell - this will take time as it calls the LLM)\n",
    "results_table = await evaluation_pipeline.run_evaluation(dataset, model, llm_client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-reflection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ASR_venv)",
   "language": "python",
   "name": "asr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
