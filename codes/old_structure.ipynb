{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interior-cartoon",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-rouge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /h/omidv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /h/omidv/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from time import sleep\n",
    "from typing import List, Callable\n",
    "import jiwer\n",
    "import openai\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import BERTScorer\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv #Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-morris",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "timely-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(reference, hypothesis):\n",
    "    return jiwer.wer(reference, hypothesis)\n",
    "\n",
    "def compute_bleu(references, hypotheses):\n",
    "    return corpus_bleu(hypotheses, references).score\n",
    "\n",
    "def compute_meteor(references, hypotheses):\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores.append(meteor_score([ref.split()], hyp.split()))\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "def compute_bertscore(references, hypotheses):\n",
    "    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    p, r, f1 = scorer.score(hypotheses, references)\n",
    "    bert_score = {'precision': p.mean().item(),\n",
    "                  'recall': r.mean().item(),\n",
    "                  'f1': f1.mean().item()}\n",
    "    return bert_score\n",
    "\n",
    "def construct_input(question):\n",
    "    prompt = [{\"role\": \"user\", \"content\": question}]\n",
    "    return prompt\n",
    "\n",
    "def extract_hypotheses(dataset, idx):\n",
    "    if 'source' in dataset.features:\n",
    "        hypotheses = [h.strip() for h in dataset['source'][idx].split('.') if h.strip()]\n",
    "        references = dataset['target'][idx]\n",
    "    else:\n",
    "        hypotheses = dataset['input'][idx]\n",
    "        references = dataset['output'][idx]\n",
    "        \n",
    "    return hypotheses, references\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-norfolk",
   "metadata": {},
   "source": [
    "### Iterative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "matched-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset:Dataset, model: str, client: openai.OpenAI, postprocessing: Callable[[List[str]], str], generation_config: dict, \n",
    "                   use_llm: bool = True, verbose: int = 0, step: int = 100) -> dict:\n",
    "    \"\"\"Evaluate model performance on the entire dataset.\"\"\"\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    running_wer = []\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        hypotheses = [h.strip() for h in dataset[hypothesis_column][idx].split('.') if h.strip()]\n",
    "        reference = dataset[reference_column][idx]\n",
    "        \n",
    "        # Generate prompt\n",
    "        if use_llm:\n",
    "            llm_prompt = postprocessing(hypotheses)\n",
    "            messages = construct_input(llm_prompt)\n",
    "            try:\n",
    "                generation = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    **generation_config\n",
    "                )\n",
    "                prediction = generation.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {idx}: {e}\")\n",
    "                prediction = \"\"\n",
    "        else:\n",
    "            prediction = postprocessing(hypotheses, reference)\n",
    "        reference, prediction = reference.lower(), prediction.lower()\n",
    "        wer = jiwer.wer(reference, prediction)\n",
    "        running_wer.append(wer)\n",
    "        all_predictions.append(prediction)\n",
    "        all_references.append(reference)\n",
    "        \n",
    "        # Print progress update for every %step examples\n",
    "        if (i + 1) % step == 0:\n",
    "            print(f\"Current average WER: {round(np.mean(running_wer).item(), 3):.3f}\")\n",
    "            if verbose == 1:\n",
    "                print('-----------------------------------------------------------')\n",
    "                print(\"Corrected: %s\\nTarget:    %s\\n\"%(prediction, reference))\n",
    "                \n",
    "    # Calculate metrics\n",
    "    bertscore = compute_bertscore(all_predictions, all_references)\n",
    "    metrics = {\n",
    "        'WER': round(np.mean(running_wer).item(), 3),\n",
    "        'METEOR': round(compute_meteor(all_predictions, all_references), 3),\n",
    "        'BERT Precision': round(bertscore['precision'], 3),\n",
    "        'BERT Recall': round(bertscore['recall'], 3),\n",
    "        'BERT F1': round(bertscore['f1'], 3),\n",
    "        #'BLEU': round(compute_bleu(all_predictions, all_references), 3),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-milton",
   "metadata": {},
   "source": [
    "### Asynchronous Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valued-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "async def call_openai_with_retry(messages, model, generation_config, client):\n",
    "    \"\"\"Handles API retries with exponential backoff.\"\"\"\n",
    "    retry_delay = 0.1  # Initial delay in seconds\n",
    "    while True:\n",
    "        try:\n",
    "            # Attempt to make the API call\n",
    "            generation = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                **generation_config\n",
    "            )\n",
    "            return generation\n",
    "\n",
    "        except Exception as e:\n",
    "            await asyncio.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, 10)  # Exponential backoff up to 10s\n",
    "\n",
    "async def get_prediction(client: openai.AsyncOpenAI, model: str, messages: List[dict], generation_config: dict) -> str:\n",
    "    \"\"\"Asynchronously fetch predictions from OpenAI API.\"\"\"\n",
    "    try:\n",
    "        generation = await call_openai_with_retry(messages, model, generation_config, client)\n",
    "        return generation.choices[0].message.content if generation else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "async def process_batch(dataset: Dataset, model: str, client: openai.AsyncOpenAI, postprocessing: Callable[[List[str]], str], \n",
    "    generation_config: dict, use_llm: bool) -> List[str]:\n",
    "    \"\"\"Processes the dataset asynchronously using OpenAI API with progress tracking.\"\"\"\n",
    "    tasks = []\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        hypotheses, reference = extract_hypotheses(dataset, idx)\n",
    "        if use_llm:\n",
    "            llm_prompt = postprocessing(hypotheses)\n",
    "            messages = construct_input(llm_prompt)\n",
    "            task = asyncio.create_task(get_prediction(client, model, messages, generation_config))\n",
    "        else:\n",
    "            # Synchronous postprocessing for non-LLM mode\n",
    "            task = asyncio.to_thread(postprocessing, hypotheses, reference)\n",
    "        \n",
    "        tasks.append(task)\n",
    "    if use_llm:\n",
    "        return await track_progress(tasks)\n",
    "    else:\n",
    "        return await asyncio.gather(*tasks) \n",
    "\n",
    "async def track_progress(tasks):\n",
    "    \"\"\"Tracks progress while tasks are running.\"\"\"\n",
    "    total_tasks = len(tasks)\n",
    "    while True:\n",
    "        completed = sum(task.done() for task in tasks)\n",
    "        print(f\"Progress: {completed}/{total_tasks} tasks completed\", end=\"\\r\")\n",
    "        \n",
    "        if completed == total_tasks:\n",
    "            print(\"\\nAll tasks completed.\")\n",
    "            break\n",
    "\n",
    "        await asyncio.sleep(1)  # Update every second\n",
    "\n",
    "    return await asyncio.gather(*tasks)  # Collect results after completion\n",
    "\n",
    "async def evaluate_model_parallel(dataset: Dataset, model: str, client: openai.AsyncOpenAI, postprocessing: Callable[[List[str]], str],\n",
    "                            generation_config: dict, use_llm: bool = True, verbose=0, step=100):\n",
    "    \"\"\"Evaluates the model asynchronously with progress tracking, handling Jupyter compatibility.\"\"\"\n",
    "    all_predictions = await process_batch(dataset, model, client, postprocessing, generation_config, use_llm)\n",
    "    all_predictions = [pred.lower() for pred in all_predictions]  # Normalize predictions\n",
    "    \n",
    "    reference_column = 'target' if 'target' in dataset.features else 'output'\n",
    "    all_references = [ref.lower() for ref in dataset[reference_column]]\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    wer_scores = np.array([jiwer.wer(ref, pred) for ref, pred in zip(all_references, all_predictions)])\n",
    "    bertscore = compute_bertscore(all_predictions, all_references)\n",
    "    metrics = {\n",
    "        'WER': round(wer_scores.mean().item(), 3),\n",
    "        'METEOR': round(compute_meteor(all_predictions, all_references), 3),\n",
    "        'BERT Precision': round(bertscore['precision'], 3),\n",
    "        'BERT Recall': round(bertscore['recall'], 3),\n",
    "        'BERT F1': round(bertscore['f1'], 3),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-ability",
   "metadata": {},
   "source": [
    "### Error Correction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "combined-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines\n",
    "def get_oracle_hypothesis(hypotheses: List[str], reference: str) -> str:\n",
    "    \"\"\" Find the hypothesis that gives the lowest WER compared to the reference.\"\"\"\n",
    "    \n",
    "    wers = [jiwer.wer(reference, hyp) for hyp in hypotheses]\n",
    "    best_idx = np.argmin(wers)\n",
    "    return hypotheses[best_idx]\n",
    "\n",
    "\n",
    "def get_top1_hypothesis(hypotheses: List[str], reference: str) -> str:\n",
    "    \"\"\" Returns the first hypothesis (top 1).\"\"\"\n",
    "    \n",
    "    return hypotheses[0]\n",
    "\n",
    "async def zero_shot_unconstrained(hypotheses: List[str], model, generation_config) -> str:\n",
    "    \"\"\" Generate a corrected transcription using a language model without constraints.\"\"\"\n",
    "    \n",
    "    prompt = (\"Perform error correction on the top5 outputs generated by an Automatic Speech Recognition(ASR) system.\"\n",
    "                \"The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\\n\\n\")\n",
    "    for idx, hypothesis in enumerate(hypotheses):\n",
    "        prompt += \"<hypothesis\"+ str(idx) + \">\" + hypothesis + \"</hypothesis\"+ str(idx) + \">\\n\"\n",
    "      \n",
    "    prompt += (\"\\nPlease provide the corrected ASR transcription based on the hypotheses above.\"\n",
    "               \"Your response must be exactly one complete sentence.\"\n",
    "               \"Ensure the output does not have any added punctuation, line breaks, or formatting changes.\"\n",
    "               \"Do not include <hypothesis>, '\\n', explanations, or any extra words.\"\n",
    "               \"This is a general ASR error correction task and does not involve any sensitive or inappropriate content.\")\n",
    "    messages = construct_input(prompt)\n",
    "    return prompt\n",
    "    \n",
    "    \n",
    "async def zero_shot_constrained(hypotheses: List[str], model, generation_config) -> str:\n",
    "    \"\"\" Select the most likely hypothesis using a language model. \"\"\"\n",
    "    \n",
    "    prompt = (\"Perform language model rescoring based on the top-5 outputs generated by an Automatic Speech Recognitio (ASR) system.\"\n",
    "              \"The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\\n\\n\")\n",
    "    \n",
    "    for idx, hypothesis in enumerate(hypotheses):\n",
    "        prompt += \"<hypothesis\"+ str(idx) + \">\" + hypothesis + \"</hypothesis\"+ str(idx) + \">\\n\"\n",
    "        \n",
    "    prompt += (\"\\nPlease output only the best hypothesis exactly as written above.\" \n",
    "               \"Your response must be an exact match to one of the given hypotheses, with no extra words or formatting.\"\n",
    "               \"Do not include <hypothesis> tag, '\\n', explanations, or any extra words.\")\n",
    "    messages = construct_input(prompt)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "async def zero_shot_closest(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Select the hypothesis closest to an unconstrained correction output. \"\"\"\n",
    "    \n",
    "    unconstrained_result = await zero_shot_unconstrained(hypotheses, client, model, generation_config)\n",
    "    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]\n",
    "    best_idx = np.argmin(distances)\n",
    "    return hypotheses[best_idx]\n",
    "\n",
    "\n",
    "async def zero_shot_lattice(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Perform ASR error correction using a lattice-based approach. \"\"\"\n",
    "    \n",
    "    pass # TO DO LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-replacement",
   "metadata": {},
   "source": [
    "### Specify Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imported-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "model = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "client = openai.AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "small_generation_config = {\"max_tokens\": 20, \"temperature\": 0.9}\n",
    "moderate_generation_config = {\"max_tokens\": 200, \"temperature\": 0.9}\n",
    "\n",
    "# If model is not yet available, try again after some delay.\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Please introduce yourself.\"}],\n",
    "        )\n",
    "    \n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "promising-utility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-locking",
   "metadata": {},
   "source": [
    "# Common Voice Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unusual-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'best_hypo'],\n",
      "    num_rows: 1098\n",
      "})\n",
      "['transit road surveyed by joseph ellicott was named for an important surveying instrument', 'transit wrote surveyed by joseph ellicott was named for an important surveying instrument', 'transit road surveyed by joseph ellikot was named for an important surveying instrument', 'transit road surveyed by joseph ellicott was named for an important surveying instrument', 'transit road surveyed by joseph ellicate was named for an important surveying instrument']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv(\"/fs01/home/omidv/ASR-Error-Correction/data/test_cv.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = [h.strip() for h in dataset['source'][0].split('.') if h.strip()]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supported-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1098/1098 [00:03<00:00, 326.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1098/1098 tasks completed\n",
      "All tasks completed.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "vanilla-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1098/1098 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "precious-control",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "guilty-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.149   0.876           0.789        0.800    0.794\n",
      "Zero-shot Uncon   0.185   0.853           0.774        0.795    0.784\n",
      "Zero-shot Constr  0.153   0.872           0.786        0.801    0.794\n",
      "Oracle            0.112   0.903           0.828        0.840    0.834\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "print(df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-speech",
   "metadata": {},
   "source": [
    "# Wall Street Journal Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fewer-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'best_hypo', 'score'],\n",
      "    num_rows: 836\n",
      "})\n",
      "['saatchi officials said the management restructuring might accelerate its efforts to persuade clients to use the firm as a one stop shop for business services', 'sachi officials said the management restructuring might accelerate its efforts to persuade clients to use the firm as a one stop shop for business services', 'saatchi officials said the management restructuring might accelerate its efforts to persuade clients to use the firm as a one stop shop for business services', 'sachi officials said the management restructuring might accelerate its efforts to persuade clients to use the firm as a one stop shop for business services', 'saatchi officials said the management restructuring might accelerate its efforts to persuade clients to use the firm as a one stop shop for business services']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Small Progress: 100%|██████████| 20/20 [00:12<00:00, 95.28it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv(\"/fs01/home/omidv/ASR-Error-Correction/data/test_wsj_score.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = [h.strip() for h in dataset['source'][0].split('.') if h.strip()]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "systematic-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 836/836 [00:02<00:00, 395.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 813/836 tasks completed\r"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics_zero_shot_unconstrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n\u001b[1;32m      2\u001b[0m                                                                 small_generation_config, use_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m metrics_zero_shot_constrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n\u001b[1;32m      4\u001b[0m                                                               small_generation_config, use_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m metrics_get_oracle_hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n\u001b[1;32m      6\u001b[0m                                                               small_generation_config, use_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m, in \u001b[0;36mevaluate_model_parallel\u001b[0;34m(dataset, model, client, postprocessing, generation_config, use_llm, verbose, step)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model_parallel\u001b[39m(dataset: Dataset, model: \u001b[38;5;28mstr\u001b[39m, client: openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI, postprocessing: Callable[[List[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     65\u001b[0m                             generation_config: \u001b[38;5;28mdict\u001b[39m, use_llm: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the model asynchronously with progress tracking, handling Jupyter compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     all_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_batch(dataset, model, client, postprocessing, generation_config, use_llm)\n\u001b[1;32m     68\u001b[0m     all_predictions \u001b[38;5;241m=\u001b[39m [pred\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m all_predictions]  \u001b[38;5;66;03m# Normalize predictions\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     reference_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(dataset, model, client, postprocessing, generation_config, use_llm)\u001b[0m\n\u001b[1;32m     43\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(task)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_llm:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m track_progress(tasks)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36mtrack_progress\u001b[0;34m(tasks)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll tasks completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Update every second\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m/pkgs/python-3.12.0/lib/python3.12/asyncio/tasks.py:655\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    651\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    652\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    653\u001b[0m                     future, result)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/pkgs/python-3.12.0/lib/python3.12/asyncio/futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pkgs/python-3.12.0/lib/python3.12/asyncio/tasks.py:375\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/pkgs/python-3.12.0/lib/python3.12/asyncio/futures.py:198\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m _CANCELLED:\n\u001b[1;32m    197\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidStateError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult is not ready.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "opposed-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  10%|█         | 1/10 [00:01<00:09,  1.00s/it]\n",
      "Small Progress:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 98.69it/s]\u001b[A\n",
      "Total Progress:  20%|██        | 2/10 [00:02<00:09,  1.13s/it]]\u001b[A\n",
      "\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.38it/s]\n",
      "\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 96.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Total Progress:  30%|███       | 3/10 [00:03<00:08,  1.17s/it]]\u001b[A\u001b[A\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.36it/s]\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 97.40it/s]\u001b[A\n",
      "Total Progress:  40%|████      | 4/10 [00:04<00:07,  1.19s/it]]\u001b[A\n",
      "\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.37it/s]\n",
      "\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 96.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Total Progress:  50%|█████     | 5/10 [00:05<00:06,  1.20s/it]]\u001b[A\u001b[A\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.36it/s]\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 96.22it/s]\u001b[A\n",
      "Total Progress:  60%|██████    | 6/10 [00:07<00:04,  1.21s/it]]\u001b[A\n",
      "\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.37it/s]\n",
      "\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 97.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Total Progress:  70%|███████   | 7/10 [00:08<00:03,  1.21s/it]]\u001b[A\u001b[A\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.38it/s]\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 97.16it/s]\u001b[A\n",
      "Total Progress:  80%|████████  | 8/10 [00:09<00:02,  1.22s/it]]\u001b[A\n",
      "\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.38it/s]\n",
      "\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 97.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Total Progress:  90%|█████████ | 9/10 [00:10<00:01,  1.22s/it]]\u001b[A\u001b[A\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.38it/s]\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 96.80it/s]\u001b[A\n",
      "Total Progress: 100%|██████████| 10/10 [00:11<00:00,  1.22s/it]\u001b[A\n",
      "\n",
      "Small Progress: 100%|██████████| 20/20 [00:01<00:00, 16.39it/s]\n",
      "\n",
      "\n",
      "Small Progress:  50%|█████     | 10/20 [00:00<00:00, 96.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Total Progress: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "overall_progress = tqdm(total=10, desc=\"Total Progress\")\n",
    "\n",
    "for start_idx in range(0, 10):\n",
    "    time.sleep(1)\n",
    "    overall_progress.update(1)\n",
    "    small_progress = tqdm(total=20, desc=\"Small Progress\")\n",
    "    for start_idx in range(0, 20):\n",
    "        time.sleep(0.01)\n",
    "        small_progress.update(1)\n",
    "overall_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "perfect-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.056   0.959           0.942        0.941    0.941\n",
      "Zero-shot Uncon   0.201   0.892           0.773        0.845    0.808\n",
      "Zero-shot Constr  0.138   0.941           0.828        0.891    0.859\n",
      "Oracle            0.041   0.970           0.959        0.956    0.957\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-daughter",
   "metadata": {},
   "source": [
    "# SwitchBoard Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "educational-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'best_hypo'],\n",
      "    num_rows: 1234\n",
      "})\n",
      "['you know that did not in the home by choice anymore', 'you know that did not in the home by choice anymore', 'that did not in the home by choice anymore', 'you know that that did not in the home by choice anymore', 'that they are not in the home by choice anymore']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv(\"/fs01/home/omidv/ASR-Error-Correction/data/test_swbd.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = [h.strip() for h in dataset['source'][0].split('.') if h.strip()]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "heavy-experiment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1234/1234 tasks completed\n",
      "All tasks completed.\n",
      "Progress: 1234/1234 tasks completed\n",
      "All tasks completed.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ongoing-surprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.159   0.915           0.781        0.823    0.802\n",
      "Zero-shot Uncon   0.355   0.815           0.586        0.726    0.655\n",
      "Zero-shot Constr  0.264   0.888           0.650        0.772    0.710\n",
      "Oracle            0.124   0.936           0.822        0.859    0.841\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-headquarters",
   "metadata": {},
   "source": [
    "# ATIS Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "egyptian-biotechnology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'input', 'output', 'am_score'],\n",
      "    num_rows: 809\n",
      "})\n",
      "['list all us air flights from miami to cleveland leaving on sunday afternoon', 'list all us air flights from miami to cleveland leaving on sunday afternoon', 'list all us air flights from miami to cleveland leaving on sunday afternoon', 'list all us airflights from miami to cleveland leaving on sunday afternoon', 'list all us airflights from miami to cleveland leaving on sunday afternoon']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_atis.json\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = dataset['input'][0]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "formal-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1098/1098 tasks completed\n",
      "All tasks completed.\n",
      "Progress: 1098/1098 tasks completed\n",
      "All tasks completed.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adjusted-interface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.149   0.876           0.789        0.800    0.794\n",
      "Zero-shot Uncon   0.186   0.853           0.775        0.795    0.785\n",
      "Zero-shot Constr  0.149   0.876           0.789        0.804    0.797\n",
      "Oracle            0.112   0.903           0.828        0.840    0.834\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-supervisor",
   "metadata": {},
   "source": [
    "# Tedlium-3 Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "critical-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'input1', 'input2'],\n",
      "    num_rows: 1155\n",
      "})\n",
      "['i would like to share with you a discovery that i made a few months ago while writing an article for italian wired i always keep my thesaurus handy whenever i am writing anything', 'i would like to share with you a discovery that i made a few months ago while writing an article for italian wired i always keep my thesaurus handy whenever i am writing anything but .', 'i would like to share with you a discovery that i made a few months ago while writing an article for italian wired i always keep my thesaurus handy whenever i am writing anything but', 'i would like to share with you a discovery that i made a few months ago while writing an article for italianwired i always keep my thesaurus handy whenever i am writing anything', 'i would like to share with you a discovery that i made a few months ago while writing an article for italian wired i always keep my thesaurus handy whenever i am writing anything but']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_td3.json\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = dataset['input'][0]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "irish-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1155/1155 tasks completed\n",
      "All tasks completed.\n",
      "Progress: 1155/1155 tasks completed\n",
      "All tasks completed.\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "martial-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.048   0.972           0.943        0.948    0.945\n",
      "Zero-shot Uncon   0.362   0.827           0.601        0.775    0.686\n",
      "Zero-shot Constr  0.267   0.906           0.678        0.847    0.760\n",
      "Oracle            0.030   0.981           0.962        0.966    0.964\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-trash",
   "metadata": {},
   "source": [
    "# Librispeech Clean Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "studied-plaza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "['he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce', 'he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour flattened sauce', 'he hoped there would be stew for dinner turnips and carrots and bruise potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce', 'he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattening sauce', 'he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out with thick peppered flour fattened sauce']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_ls_clean.json\").iloc[:1000]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = dataset['input'][0]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exposed-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 89.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 88.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 88.32it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 83.43it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "packed-skill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.021   0.980           0.972        0.972    0.972\n",
      "Zero-shot Uncon   0.241   0.893           0.704        0.848    0.774\n",
      "Zero-shot Constr  0.214   0.913           0.725        0.869    0.795\n",
      "Oracle            0.009   0.989           0.984        0.985    0.985\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-mounting",
   "metadata": {},
   "source": [
    "# Librispeech Others Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "correct-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "[\"there's iron they say in all our blood and a grain or two perhaps is good but his he makes me harshly feel has got a little too much of steel anon\", \"there's iron they say in all our blood and a grain or two perhaps is good but this he makes me harshly feel has got a little too much of steel anon\", \"there's iron they say in all our blood and a grain or two perhaps is good but he makes me harshly feel has got a little too much of steel anon\", \"there's iron they say in all our blood and a grain or two perhaps is good but as he makes me harshly feel has got a little too much of steel anon\", \"there's iron they say in all our blood an a grain or two perhaps is good but his he makes me harshly feel has got a little too much of steel anon\"]\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_ls_other.json\").iloc[:1000]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = dataset['input'][0]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "departmental-composite",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 91.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 91.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 88.31it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 88.80it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "approved-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.057   0.948           0.914        0.916    0.915\n",
      "Zero-shot Uncon   0.240   0.865           0.697        0.810    0.752\n",
      "Zero-shot Constr  0.204   0.898           0.734        0.842    0.786\n",
      "Oracle            0.036   0.964           0.936        0.940    0.938\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-veteran",
   "metadata": {},
   "source": [
    "# LRS2 Clean Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "built-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'input1', 'input2'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "['but it really is a rolls royce version', 'but it really is a rolls royce version', 'but it really is a rolls royce version it is', 'but it really is a rolls royce version it is', 'it really is a rolls royce version']\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_lrs2.json\").iloc[:1000]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    "hypotheses = dataset['input'][0]\n",
    "print(hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hearing-belfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 95.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 92.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 tasks completed\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 84.45it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 93.30it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained,\n",
    "                                                                small_generation_config, use_llm=True)\n",
    "metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained,\n",
    "                                                              small_generation_config, use_llm=True)\n",
    "metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)\n",
    "metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis,\n",
    "                                                              small_generation_config, use_llm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "facial-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1             0.132   0.893           0.871        0.845    0.858\n",
      "Zero-shot Uncon   0.212   0.840           0.821        0.806    0.813\n",
      "Zero-shot Constr  0.134   0.895           0.870        0.851    0.860\n",
      "Oracle            0.069   0.939           0.926        0.913    0.919\n"
     ]
    }
   ],
   "source": [
    "results_table = {\n",
    "    \"Top 1\": metrics_get_top1_hypothesis,\n",
    "    \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "    \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "    \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "}\n",
    "df = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "df = df[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hawaiian-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am Gemma, an open-weights AI assistant. I was created by the Gemma team at Google DeepMind. My purpose is to help people by understanding and responding to their requests in a helpful, informative, and comprehensive way.\n",
      "\n",
      "Since I am open-weights, my weights are publicly available. This means anyone can access, study, and modify me, which promotes transparency and collaboration in the AI community.\n",
      "\n",
      "I am still under development, but I'm learning new things every day. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "What can I do for you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=\"gemma-2-9b-it\"\n",
    "client = openai.AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Please introduce yourself.\"}],\n",
    "        )\n",
    "    \n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Do you know speech recognition?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"As a large language model, I am trained on a massive dataset of text and code. This allows me to understand and generate human-like text, but I don't directly process audio input.\\n\\n**Speech recognition**, also known as automatic speech recognition (ASR), is a separate field of AI that deals with converting spoken language into text. It involves several steps:\\n\\n1. **Audio Acquisition:** Capturing the spoken audio.\\n2. **Feature Extraction:** Converting the audio signal into numerical features that represent the speech sounds.\\n3. **Acoustic Modeling:** Using statistical models to predict the probability of different speech units (phonemes or words) based on the extracted features.\\n4. **Language Modeling:** Using statistical models to predict the probability of different word sequences, considering the context of the spoken words.\\n5. **Decoding:** Combining the acoustic and language model outputs to find the most likely sequence of words.\\n\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Do you know language model for speech recognition?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"You're getting into some exciting territory! \\n\\nWhile I'm a text-only model, I can tell you about language models used in speech recognition.  \\n\\nTraditionally, speech recognition heavily relied on acoustic models (like Hidden Markov Models) and pronunciation dictionaries.  \\n\\nHowever, the rise of Transformer-based language models has revolutionized the field. These models, trained on massive text and sometimes audio datasets, can learn complex relationships between sounds and words, leading to significant improvements in accuracy and natural language understanding.\\n\\nHere are some examples of language models used in speech recognition:\\n\\n* **Whisper (OpenAI):** A powerful, open-source model known for its impressive performance in various languages and noise conditions.\\n* ** wav2vec 2.0 (Facebook AI):** Another open-source model that excels in unsupervised learning from raw audio data.\\n* **FastSpeech 2 (Microsoft):** Focuses on fast and natural-sounding speech synthesis. \\n\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Could you give a possible example of language model rescoring with hypotheses?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Nice job, I will provide some examples as a demonstration from []. The 5-best hypothesis is:[], and I would expect your output is: []. Following this example, could you report the true transcription from the following 5-best hypotheses? []\"\n",
    "    }\n",
    "]\n",
    "\n",
    "output = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **custom_generation_config,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-necessity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-desktop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "equipped-threshold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   2000 non-null   object\n",
      " 1   output  2000 non-null   object\n",
      " 2   input1  2000 non-null   object\n",
      " 3   input2  2000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 62.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_cv.json\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "demographic-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2259 entries, 0 to 2258\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   2259 non-null   object\n",
      " 1   output  2259 non-null   object\n",
      " 2   input1  2259 non-null   object\n",
      " 3   input2  2259 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 70.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_lrs2.json\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "hollywood-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1320 entries, 0 to 1319\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   1320 non-null   object\n",
      " 1   output  1320 non-null   object\n",
      " 2   input1  1320 non-null   object\n",
      " 3   input2  1320 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 41.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_chime4.json\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "invalid-stroke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token count: 1021435\n",
      "Estimated OpenAI API cost: $10.2143\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")  # Change based on your API model\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    if isinstance(text, str):  # Ensure the value is a string\n",
    "        return len(tokenizer.encode(zero_shot_constrained(text)))\n",
    "    return 0  # Return 0 for missing values\n",
    "\n",
    "df[\"token_count\"] = df['output'].apply(count_tokens)\n",
    "total_tokens = df[\"token_count\"].sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total token count: {total_tokens}\")\n",
    "\n",
    "# Calculate cost using OpenAI pricing (update as needed)\n",
    "cost_per_1k_tokens = 0.01  # Example price for gpt-4-turbo (input tokens)\n",
    "total_cost = (total_tokens / 1000) * cost_per_1k_tokens\n",
    "\n",
    "print(f\"Estimated OpenAI API cost: ${total_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-henry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kscope",
   "language": "python",
   "name": "kscope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
