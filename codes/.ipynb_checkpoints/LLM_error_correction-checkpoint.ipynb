{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interior-cartoon",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worth-rouge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /h/omidv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /h/omidv/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "import random\n",
    "from time import sleep\n",
    "from typing import List, Callable\n",
    "import jiwer\n",
    "import openai\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import BERTScorer\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv #Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-morris",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "timely-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(reference, hypothesis):\n",
    "    return jiwer.wer(reference, hypothesis)\n",
    "\n",
    "def compute_bleu(references, hypotheses):\n",
    "    return corpus_bleu(hypotheses, references).score\n",
    "\n",
    "def compute_meteor(references, hypotheses):\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores.append(meteor_score([ref.split()], hyp.split()))\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "def compute_bertscore(references, hypotheses):\n",
    "    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    p, r, f1 = scorer.score(hypotheses, references)\n",
    "    bert_score = {'precision': p.mean().item(),\n",
    "                  'recall': r.mean().item(),\n",
    "                  'f1': f1.mean().item()}\n",
    "    return bert_score\n",
    "\n",
    "def compute_levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Compute the Levenshtein distance between two strings.\"\"\"\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    dp = np.zeros((len_s1 + 1, len_s2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(len_s1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1 + 1):\n",
    "        for j in range(1, len_s2 + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,      # Deletion\n",
    "                           dp[i][j - 1] + 1,      # Insertion\n",
    "                           dp[i - 1][j - 1] + cost)  # Substitution\n",
    "\n",
    "    return dp[len_s1][len_s2]\n",
    "\n",
    "def construct_input(question):\n",
    "    prompt = [{\"role\": \"user\", \"content\": question}]\n",
    "    return prompt\n",
    "\n",
    "def extract_hypotheses(dataset, idx):\n",
    "    if 'source' in dataset.features:\n",
    "        hypotheses = [h.strip() for h in dataset['source'][idx].split('.') if h.strip()]\n",
    "        references = dataset['target'][idx]\n",
    "    else:\n",
    "        hypotheses = dataset['input'][idx]\n",
    "        references = dataset['output'][idx]\n",
    "        \n",
    "    return hypotheses, references\n",
    "\n",
    "def save_results(dataset: Dataset, corrections: list, model_name: str, function_name: str, file_path: str):\n",
    "    \n",
    "    correction_column = f\"corrected_by_{model_name}_{function_name}\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        existing_df = pd.read_json(file_path)\n",
    "    else:\n",
    "        existing_df = dataset.to_pandas()\n",
    "\n",
    "    if correction_column not in existing_df.columns:\n",
    "        existing_df[correction_column] = None\n",
    "\n",
    "    existing_df[correction_column] = corrections\n",
    "    existing_df.to_json(file_path, orient=\"records\", indent=4)\n",
    "    print(f\"Results saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "educated-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation(dataset, model, client, generation_config, results_path):\n",
    "    print(\"Evaluating Zero-shot Unconstrained:\")\n",
    "    metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained, generation_config, results_path)\n",
    "    \n",
    "    print(\"Evaluating Zero-shot Constrained:\")\n",
    "    metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_constrained, generation_config, results_path)\n",
    "    \n",
    "    print(\"Evaluating Zero-shot Closest:\")\n",
    "    metrics_zero_shot_constrained = await evaluate_model_parallel(dataset, model, client, zero_shot_closest, generation_config, results_path)\n",
    "    \n",
    "    print(\"Evaluating Oracle:\")\n",
    "    metrics_get_oracle_hypothesis = await evaluate_model_parallel(dataset, model, client, get_oracle_hypothesis, generation_config, results_path)\n",
    "    \n",
    "    print(\"Evaluating Top 1:\")\n",
    "    metrics_get_top1_hypothesis = await evaluate_model_parallel(dataset, model, client, get_top1_hypothesis, generation_config, results_path)\n",
    "\n",
    "    results_table = {\n",
    "        \"Top 1\": metrics_get_top1_hypothesis,\n",
    "        \"Zero-shot Uncon\": metrics_zero_shot_unconstrained,\n",
    "        \"Zero-shot Constr\": metrics_zero_shot_constrained,\n",
    "        \"Zero-shot Closest\": metrics_zero_shot_constrained,\n",
    "        \"Oracle\": metrics_get_oracle_hypothesis,\n",
    "    }\n",
    "    results_table = pd.DataFrame.from_dict(results_table, orient='index')\n",
    "    results_table = results_table[['WER', 'METEOR', 'BERT Precision', 'BERT Recall', 'BERT F1']]\n",
    "\n",
    "    # Save as JSON\n",
    "    csv_path = results_path.replace(\".json\", \".csv\")\n",
    "    results_table.to_csv(csv_path)\n",
    "    print(f\"Benchmark saved to {csv_path}\")\n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-milton",
   "metadata": {},
   "source": [
    "### Asynchronous Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "valued-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def call_openai_with_retry(messages, model, generation_config, client):\n",
    "    \"\"\"Handles API retries with exponential backoff.\"\"\"\n",
    "    \n",
    "    retry_delay = 0.1  # Initial delay in seconds\n",
    "    max_delay = 10\n",
    "    while True:\n",
    "        try:\n",
    "            # Attempt to make the API call\n",
    "            generation = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                **generation_config\n",
    "            )\n",
    "            return generation\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            wait_time = retry_delay\n",
    "            if hasattr(e, \"response\") and e.response is not None:\n",
    "                try:\n",
    "                    error_data = e.response.json()\n",
    "                    wait_time = float(error_data.get(\"detail\", {}).get(\"wait_seconds\", {}))\n",
    "                except:\n",
    "                    pass\n",
    "            await asyncio.sleep(wait_time + 0.1)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            await asyncio.sleep(retry_delay)\n",
    "            retry_delay = min(retry_delay * 2, max_delay)  # Exponential backoff\n",
    "\n",
    "\n",
    "async def get_prediction(client: openai.AsyncOpenAI, model: str, messages: List[dict], generation_config: dict) -> str:\n",
    "    \"\"\"Asynchronously fetch predictions from OpenAI API.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        generation = await call_openai_with_retry(messages, model, generation_config, client)\n",
    "        return generation.choices[0].message.content if generation else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "async def track_progress(tasks):\n",
    "    \"\"\"Tracks progress while tasks are running.\"\"\"\n",
    "    \n",
    "    total_tasks = len(tasks)\n",
    "    while not all(task.done() for task in tasks):\n",
    "        completed = sum(task.done() for task in tasks)\n",
    "        print(f\"Progress: {completed}/{total_tasks} tests completed!\", end=\"\\r\")\n",
    "        await asyncio.sleep(0.1)\n",
    "    print(f\"Progress: Batch of {total_tasks} tests completed!\", flush=True)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "    \n",
    "async def process_batch(dataset: Dataset, indices: List[int], model: str, client: openai.AsyncOpenAI, postprocessing: Callable[[List[str]], str], \n",
    "    generation_config: dict) -> List[str]:\n",
    "    \"\"\"Processes the dataset asynchronously using OpenAI API with progress tracking.\"\"\"\n",
    "    \n",
    "    tasks = []\n",
    "    for idx in indices:\n",
    "        hypotheses, reference = extract_hypotheses(dataset, idx)\n",
    "        \n",
    "        if inspect.iscoroutinefunction(postprocessing):\n",
    "            tasks.append(asyncio.create_task(postprocessing(hypotheses, client, model, generation_config)))\n",
    "        else:\n",
    "            tasks.append(asyncio.create_task(asyncio.to_thread(postprocessing, hypotheses, reference)))\n",
    "    \n",
    "    results = await track_progress(tasks)\n",
    "    return results\n",
    "    \n",
    "async def evaluate_model_parallel(dataset: Dataset, model: str, client: openai.AsyncOpenAI, postprocessing: Callable[[List[str]], str],\n",
    "                            generation_config: dict, results_path: str, step=256):\n",
    "    \"\"\"Evaluates the model asynchronously with progress tracking, handling Jupyter compatibility.\"\"\"\n",
    "    \n",
    "    total_rows = len(dataset)\n",
    "    all_predictions = []\n",
    "    \n",
    "    for start in range(0, total_rows, step):\n",
    "        end = min(start + step, total_rows)\n",
    "        batch_indices = list(range(start, end))\n",
    "        batch_predictions = await process_batch(dataset, batch_indices, model, client, postprocessing, generation_config)\n",
    "        all_predictions.extend(batch_predictions)\n",
    "\n",
    "    all_references = dataset['target'] if 'target' in dataset.features else dataset['output']\n",
    "    \n",
    "    # Normalize for evaluation\n",
    "    all_predictions = [pred.lower() for pred in all_predictions] \n",
    "    all_references = [ref.lower() for ref in all_references]\n",
    "\n",
    "    # Print 3 random results for manual review\n",
    "    random_indices = random.sample(range(len(all_predictions)), 3)\n",
    "    for idx in random_indices:\n",
    "        print(f\"Sample {idx + 1}\")\n",
    "        print(f\"Target: {all_references[idx]}\")\n",
    "        print(f\"Pred:   {all_predictions[idx]}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    save_results(dataset, all_predictions, model, postprocessing.__name__, results_path)\n",
    "        \n",
    "    # Compute evaluation metrics\n",
    "    wer_scores = np.array([jiwer.wer(ref, pred) for ref, pred in zip(all_references, all_predictions)])\n",
    "    bertscore = compute_bertscore(all_predictions, all_references)\n",
    "    metrics = {\n",
    "        'WER': round(wer_scores.mean().item(), 3),\n",
    "        'METEOR': round(compute_meteor(all_predictions, all_references), 3),\n",
    "        'BERT Precision': round(bertscore['precision'], 3),\n",
    "        'BERT Recall': round(bertscore['recall'], 3),\n",
    "        'BERT F1': round(bertscore['f1'], 3),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-ability",
   "metadata": {},
   "source": [
    "### Error Correction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "combined-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines\n",
    "def get_oracle_hypothesis(hypotheses: List[str], reference: str) -> str:\n",
    "    \"\"\" Find the hypothesis that gives the lowest WER compared to the reference.\"\"\"\n",
    "    \n",
    "    wers = [jiwer.wer(reference, hyp) for hyp in hypotheses]\n",
    "    best_idx = np.argmin(wers)\n",
    "    return hypotheses[best_idx]\n",
    "\n",
    "def get_top1_hypothesis(hypotheses: List[str], reference: str) -> str:\n",
    "    \"\"\" Returns the first hypothesis (top 1).\"\"\"\n",
    "    \n",
    "    return hypotheses[0]\n",
    "\n",
    "async def zero_shot_unconstrained(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Generate a corrected transcription using a language model without constraints.\"\"\"\n",
    "    \n",
    "    prompt = (\"Perform error correction on the top5 outputs generated by an Automatic Speech Recognition(ASR) system.\"\n",
    "                \"The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\\n\\n\")\n",
    "    for idx, hypothesis in enumerate(hypotheses):\n",
    "        prompt += \"<hypothesis\"+ str(idx) + \">\" + hypothesis + \"</hypothesis\"+ str(idx) + \">\\n\"\n",
    "      \n",
    "    prompt += (\"\\nPlease provide the corrected ASR transcription based on the hypotheses above.\"\n",
    "               \"Your response must be exactly one complete sentence.\"\n",
    "               \"Ensure the output does not have any added punctuation, line breaks, or formatting changes.\"\n",
    "               \"Do not include <hypothesis>, '\\n', explanations, or any extra words.\"\n",
    "               \"This is a general ASR error correction task and does not involve any sensitive or inappropriate content.\")\n",
    "    messages = construct_input(prompt)\n",
    "    return await get_prediction(client, model, messages, generation_config)\n",
    "    \n",
    "async def zero_shot_constrained(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Select the most likely hypothesis using a language model. \"\"\"\n",
    "    \n",
    "    prompt = (\"Perform language model rescoring based on the top-5 outputs generated by an Automatic Speech Recognitio (ASR) system.\"\n",
    "              \"The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\\n\\n\")\n",
    "    \n",
    "    for idx, hypothesis in enumerate(hypotheses):\n",
    "        prompt += \"<hypothesis\"+ str(idx) + \">\" + hypothesis + \"</hypothesis\"+ str(idx) + \">\\n\"\n",
    "        \n",
    "    prompt += (\"\\nPlease output only the best hypothesis exactly as written above.\" \n",
    "               \"Your response must be an exact match to one of the given hypotheses, with no extra words or formatting.\"\n",
    "               \"Do not include <hypothesis> or any additional tags in your response.\")\n",
    "    messages = construct_input(prompt)\n",
    "    return await get_prediction(client, model, messages, generation_config)\n",
    "\n",
    "async def zero_shot_closest(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Select the hypothesis closest to an unconstrained correction output. \"\"\"\n",
    "    \n",
    "    unconstrained_result = await zero_shot_unconstrained(hypotheses, client, model, generation_config)\n",
    "    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]\n",
    "    best_idx = np.argmin(distances)\n",
    "    return hypotheses[best_idx]\n",
    "\n",
    "\n",
    "async def CoT_task_activating(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Perform ASR error correction using Chain-of-Thought (CoT) reasoning.\"\"\"\n",
    "    \n",
    "    pass # TO DO\n",
    "\n",
    "async def zero_shot_lattice(hypotheses: List[str], client, model, generation_config) -> str:\n",
    "    \"\"\" Perform ASR error correction using a lattice-based approach.\"\"\"\n",
    "    \n",
    "    pass # TO DO LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "innocent-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Batch of 256 tests completed!\n",
      "Progress: Batch of 256 tests completed!\n",
      "Progress: Batch of 256 tests completed!\n",
      "Progress: Batch of 256 tests completed!\n",
      "Progress: Batch of 74 tests completed!\n",
      "Sample 203\n",
      "Target: when he died lamb wrote to william wordsworth there is captain burney gone\n",
      "Pred:   when he died lamro to william wordsworth there is captain bernie gone\n",
      "--------------------------------------------------\n",
      "Sample 833\n",
      "Target: her body taken across the lafan sands to the franciscan friary at llanfaes anglesey\n",
      "Pred:   her body taken across the laban sense to the french skin fury at the end face angle says\n",
      "--------------------------------------------------\n",
      "Sample 136\n",
      "Target: the saint george leagues club is located nearby on princes highway\n",
      "Pred:   the same george leagues club is located nearby on princess highway\n",
      "--------------------------------------------------\n",
      "Results saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\n",
      "{'WER': 0.167, 'METEOR': 0.863, 'BERT Precision': 0.792, 'BERT Recall': 0.806, 'BERT F1': 0.799}\n"
     ]
    }
   ],
   "source": [
    "metrics_zero_shot_unconstrained = await evaluate_model_parallel(dataset, model, client, zero_shot_unconstrained, small_generation_config, results_path)\n",
    "print(metrics_zero_shot_unconstrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-replacement",
   "metadata": {},
   "source": [
    "### Specify Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "imported-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "model = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "client = openai.AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "small_generation_config = {\"max_tokens\": 20, \"temperature\": 0.9}\n",
    "moderate_generation_config = {\"max_tokens\": 200, \"temperature\": 0.9}\n",
    "\n",
    "# If model is not yet available, try again after some delay.\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Please introduce yourself.\"}],\n",
    "        )\n",
    "    \n",
    "    except openai.APIError as e:\n",
    "        print(e)\n",
    "        sleep(10)\n",
    "\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-locking",
   "metadata": {},
   "source": [
    "# Common Voice Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "unusual-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'best_hypo'],\n",
      "    num_rows: 1098\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/fs01/home/omidv/ASR-Error-Correction/data/test_cv.csv\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hydraulic-union",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Zero-shot Unconstrained:\n",
      "Progress: 1000 batch completed!ted!\n",
      "Progress: 98 batch completed!ed!\n",
      "Sample 666\n",
      "Target: belmont holds the distinction of owning the world is only purpose built private subway car\n",
      "Pred:   bellman holds the distinction of owning the world's only purpose-built private supercar\n",
      "--------------------------------------------------\n",
      "Sample 257\n",
      "Target: the additional platform was on the south facing side of what is now platform two\n",
      "Pred:   the additional platform was on the south facing side of what is now platform two\n",
      "--------------------------------------------------\n",
      "Sample 362\n",
      "Target: the doctrine of indoor management is an exception to this rule\n",
      "Pred:   the doctrine of indoor management is an exception to this rule\n",
      "--------------------------------------------------\n",
      "Results saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\n",
      "Evaluating Zero-shot Constrained:\n",
      "Progress: 1000 batch completed!ted!\n",
      "Progress: 98 batch completed!ed!\n",
      "Sample 920\n",
      "Target: london bound trains head south to neighboring tadworth then turn east and finally north east\n",
      "Pred:   london bound trains head south to lebring tadworth then turn east and fall in north east\n",
      "--------------------------------------------------\n",
      "Sample 394\n",
      "Target: his six older brothers were all taught to play the fiddle by his father\n",
      "Pred:   his six older brothers were all taught to play the fiddle by his father\n",
      "--------------------------------------------------\n",
      "Sample 259\n",
      "Target: she is professor emerita of literature and writing at hampshire college\n",
      "Pred:   she is professor emerita of literature and writing at hampshire college\n",
      "--------------------------------------------------\n",
      "Results saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\n",
      "Evaluating Zero-shot Closest:\n",
      "Progress: 1000 batch completed!ted!\n",
      "Progress: 98 batch completed!ed!\n",
      "Sample 37\n",
      "Target: he started working in the coal mines at the age of nine\n",
      "Pred:   he started working in the coal mines at the age of nine\n",
      "--------------------------------------------------\n",
      "Sample 1002\n",
      "Target: and this is exactly the purpose of gorgias encomium of helen\n",
      "Pred:   and this is exactly the purpose of gorgias encomium of helen\n",
      "--------------------------------------------------\n",
      "Sample 1037\n",
      "Target: such students are not eligible for a loan from the students loan company either\n",
      "Pred:   such students are not eligible for a loan from the students loan company either\n",
      "--------------------------------------------------\n",
      "Results saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\n",
      "Evaluating Oracle:\n",
      "Progress: 1000 batch completed!ted!\n",
      "Progress: 98 batch completed!d!\n",
      "Sample 865\n",
      "Target: most of the report was publicly dismissed by the government of the day\n",
      "Pred:   most of the report publicly dismissed by the government of the day\n",
      "--------------------------------------------------\n",
      "Sample 54\n",
      "Target: it is separated into three different sections member agreement community guidelines and privacy policy\n",
      "Pred:   it is separated into three different sections member agreement community guidelines and privacy policy\n",
      "--------------------------------------------------\n",
      "Sample 728\n",
      "Target: passages through caves are often blocked by a submerged section or sump\n",
      "Pred:   passages through caves are often blocked by a submerged section or sump\n",
      "--------------------------------------------------\n",
      "Results saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\n",
      "Evaluating Top 1:\n",
      "Progress: 1000 batch completed!d!\n",
      "Progress: 98 batch completed!d!\n",
      "Sample 299\n",
      "Target: about halfway into the movement a contrasting theme is introduced which moves in minims\n",
      "Pred:   about halfway into the movement a contrasting theme is introduced which moves in minims\n",
      "--------------------------------------------------\n",
      "Sample 58\n",
      "Target: the council meets in various formations where its composition depends on the topic discussed\n",
      "Pred:   the council meets in various formations where its composition depends on the topic discussed\n",
      "--------------------------------------------------\n",
      "Sample 206\n",
      "Target: in newfoundland steve points out that they were heading in the wrong direction\n",
      "Pred:   in newfoundland steve points out that they were heading in the wrong direction\n",
      "--------------------------------------------------\n",
      "Results saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.json\n",
      "Benchmark saved to /fs01/home/omidv/ASR-Error-Correction/results/test_cv.csv\n",
      "                     WER  METEOR  BERT Precision  BERT Recall  BERT F1\n",
      "Top 1              0.149   0.876           0.789        0.800    0.794\n",
      "Zero-shot Uncon    0.258   0.802           0.735        0.713    0.724\n",
      "Zero-shot Constr   0.137   0.887           0.807        0.819    0.813\n",
      "Zero-shot Closest  0.137   0.887           0.807        0.819    0.813\n",
      "Oracle             0.112   0.903           0.828        0.840    0.834\n"
     ]
    }
   ],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-speech",
   "metadata": {},
   "source": [
    "# Wall Street Journal Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/fs01/home/omidv/ASR-Error-Correction/data/test_wsj_score.csv\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_wsj_score.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-daughter",
   "metadata": {},
   "source": [
    "# SwitchBoard Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/fs01/home/omidv/ASR-Error-Correction/data/test_swbd.csv\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_swbd.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-experiment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-headquarters",
   "metadata": {},
   "source": [
    "# ATIS Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_atis.json\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_atis.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-supervisor",
   "metadata": {},
   "source": [
    "# Tedlium-3 Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_td3.json\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_td3.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-trash",
   "metadata": {},
   "source": [
    "# Librispeech Clean Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_ls_clean.json\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_ls_clean.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-mounting",
   "metadata": {},
   "source": [
    "# Librispeech Others Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_ls_other.json\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_ls_other.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-veteran",
   "metadata": {},
   "source": [
    "# LRS2 Clean Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/fs01/home/omidv/ASR-Error-Correction/data/test_lrs2.json\")\n",
    "results_path = \"/fs01/home/omidv/ASR-Error-Correction/results/test_lrs2.json\"\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"gemma-2-9b-it\"\n",
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Mistral-7B-Instruct-v0.3\"\n",
    "results_table = await run_evaluation(dataset, model, client, small_generation_config, results_path)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-necessity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-desktop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-threshold",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-stroke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "agreed-watson",
   "metadata": {},
   "source": [
    "# Few-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-message",
   "metadata": {},
   "source": [
    "We'll start by prompting the model to solve some word problems and build up to using the Few-Shot CoT method proposed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-ghana",
   "metadata": {},
   "source": [
    "First try \"zero-shot prompting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "worse-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = (\n",
    "    \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? just give final answer with no explanation.\"\n",
    ")\n",
    "zero_shot_prompt = construct_input(zero_shot_prompt)\n",
    "\n",
    "generation_example = client.chat.completions.create(model=model,messages = zero_shot_prompt, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-orbit",
   "metadata": {},
   "source": [
    "The correct answer is 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-adventure",
   "metadata": {},
   "source": [
    "Now let's try standard few-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "approximate-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: Benjamin is taking bottle inventory. He has two cases with 15 bottles in each and one with 7. How many bottles are there in total?\n",
      "A: The answer is 37.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The answer is ...\n",
      "Just give the final answer to the last question with no explanation.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: Benjamin is taking bottle inventory. He has two cases with \"\n",
    "    \"15 bottles in each and one with 7. How many bottles are there in total?\\nA: The answer is 37.\\n\\nQ: The \"\n",
    "    \"cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\nA: \"\n",
    "    \"The answer is ...\\nJust give the final answer to the last question with no explanation.\"\n",
    ")\n",
    "few_shot_message = construct_input(few_shot_prompt)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "vietnamese-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_message,  **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-recorder",
   "metadata": {},
   "source": [
    "Now, let's try prompting the model with a few-shot CoT prompt, where we provide an example of the kind of reasoning required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "appreciated-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. \"\n",
    "    \"5 + 6 = 11. The answer is 11.\\n\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 \"\n",
    "    \"more, how many apples do they have?\\nA:\"\n",
    ")\n",
    "\n",
    "few_shot_cot_prompt_message = construct_input(few_shot_cot_prompt)\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "coastal-hacker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the problem, follow these steps:\n",
      "\n",
      "1. Start with the initial number of apples: 23\n",
      "2. Subtract the number of apples used for lunch: 23 - 20 = 3\n",
      "3. Add the number of apples bought: 3 + 6 = 9\n",
      "\n",
      "The cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_cot_prompt_message, **moderate_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-empire",
   "metadata": {},
   "source": [
    "## An example from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-playing",
   "metadata": {},
   "source": [
    "Let's try to compare few-shot prompting with few-shot CoT for slightly different kind of problem. This example is drawn from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "young-parade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these\n",
      "A: \n",
      " what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of \"\n",
    "    \"the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these\\nA: \\n what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.\"\n",
    ")\n",
    "few_shot_prompt_message = construct_input(few_shot_prompt)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "voluntary-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(c)\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_prompt_message, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-mechanics",
   "metadata": {},
   "source": [
    "The correct choice for this problem is \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "human-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these \n",
      " what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers \"\n",
    "    \"is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: If 10 is added to each number, then the mean of the \"\n",
    "    \"numbers also increases by 10. So the new mean would be 50. The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these \\n what is the answer to the second Q with a,b,c,d,or e and don't say a single word except the final answer.:\"\n",
    ")\n",
    "few_shot_cot_prompt_message = construct_input(few_shot_cot_prompt)\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "following-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_cot_prompt_message, **moderate_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-compilation",
   "metadata": {},
   "source": [
    "Sometimes the examples are not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-mongolia",
   "metadata": {},
   "source": [
    "# Zero-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-commonwealth",
   "metadata": {},
   "source": [
    "It can be tedious and tricky to form useful and effective reasoning examples. Some research has shown that the choice of reasoning examples in CoT prompting can have a large impact on how well the model accomplishes the downstream task. So let's try a zero-shot CoT approach devised in \"Large Language Models are Zero-Shot Reasoners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "disabled-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would be in each group?\n",
      "A: \n",
      "Just give the final answer to the last question with no explanation.\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: There are 64 students trying out for the school's trivia \"\n",
    "    \"teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would \"\n",
    "    \"be in each group?\\nA: \\nJust give the final answer to the last question with no explanation.\"\n",
    ")\n",
    "\n",
    "\n",
    "few_shot_prompt_message = construct_input(few_shot_prompt)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "martial-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model,messages = few_shot_prompt_message, **small_generation_config)\n",
    "print(generation_example.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-hebrew",
   "metadata": {},
   "source": [
    "The correct answer to this problem is 7.\n",
    "\n",
    "Could you get the correct answer with this example?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-nation",
   "metadata": {},
   "source": [
    "# TASK: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-division",
   "metadata": {},
   "source": [
    "Try to do CoT without adding examples.\n",
    "\n",
    "\n",
    "Split into two stages:\n",
    "\n",
    "1) Reasoning Generation   \n",
    "2) Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "annual-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams.If 36 of them didn't get picked for the team and the rest were put into 4 groups, how manystudents would be in each group?\n",
      "A: \n",
      "Let's think step by step.\n"
     ]
    }
   ],
   "source": [
    "question_prompt = (\"Q: There are 64 students trying out for the school's trivia teams.\"\n",
    "                   \"If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many\"\n",
    "                   \"students would be in each group?\\nA: \\nLet's think step by step.\" )\n",
    "\n",
    "question_prompt_message = construct_input(question_prompt)\n",
    "print(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "occupied-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Determine the number of students who were picked for the team. There are 64 students total, and 36 didn’t get picked, which means that 64 - 36 = 28 students were picked for the team.\n",
      "\n",
      "Step 2: Divide the number of students who were picked for the team by the number of groups they were put into to determine the number of students in each group. Since there are 4 groups, we need to divide 28 by 4.\n",
      "\n",
      "28 ÷ 4 = 7\n",
      "\n",
      "The final answer is: $\\boxed{7}$\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model, messages = question_prompt_message, **moderate_generation_config)\n",
    "reasoning_extraction = generation_example.choices[0].message.content\n",
    "print(reasoning_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-tunisia",
   "metadata": {},
   "source": [
    "Try to get the correct answer (7) with no example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "featured-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: break down the problem into two steps. First, ask the model for reasoning,\n",
    "# then, given the reasoning, ask for the final answer by appending \"\\nTherefore, the answer is\" followed by the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "scientific-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams.If 36 of them didn't get picked for the team and the rest were put into 4 groups, how manystudents would be in each group?\n",
      "A: Let's think step by step.\n",
      "\n",
      "Step 1: Determine the number of students who were picked for the team. There are 64 students total, and 36 didn’t get picked, which means that 64 - 36 = 28 students were picked for the team.\n",
      "\n",
      "Step 2: Divide the number of students who were picked for the team by the number of groups they were put into to determine the number of students in each group. Since there are 4 groups, we need to divide 28 by 4.\n",
      "\n",
      "28 ÷ 4 = 7\n",
      "\n",
      "The final answer is: $\\boxed{7}$\n",
      "\n",
      "Therefore, what is the final answer in numerals? Don't say a single word except the final answer.\n"
     ]
    }
   ],
   "source": [
    "reasoning_prompt = (\"Q: There are 64 students trying out for the school's trivia teams.\"\n",
    "                   \"If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many\"\n",
    "                   \"students would be in each group?\\nA: Let's think step by step.\\n\\n\" + reasoning_extraction +\n",
    "                   \"\\n\\nTherefore, what is the final answer in numerals? Don't say a single word except the final answer.\")\n",
    "\n",
    "reasoning_prompt_message = construct_input(reasoning_prompt)\n",
    "print(reasoning_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "functioning-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "generation_example = client.chat.completions.create(model=model, messages = reasoning_prompt_message, **moderate_generation_config)\n",
    "reasoning_generation = generation_example.choices[0].message.content\n",
    "print(reasoning_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-honduras",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-tuesday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-benjamin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kscope",
   "language": "python",
   "name": "kscope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
