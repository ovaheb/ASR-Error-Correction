from codes.utils import *

def get_oracle_hypothesis(hypotheses, reference):
    """ Find the hypothesis that gives the lowest WER compared to the reference."""
    
    wers = [jiwer.wer(reference, hyp) for hyp in hypotheses]
    best_idx = np.argmin(wers)
    return hypotheses[best_idx]

def get_compositional_oracle_hypothesis(hypotheses, reference):
    ref_words = reference.split()
    nbest_tokens = [hyp.split() for hyp in hypotheses]
    
    # Find the optimal word at each position by minimizing WER
    oracle_hyp = []
    
    # For each position in the reference
    for i in range(len(ref_words)):
        # Collect all tokens that appear at this position in the n-best hypotheses
        candidates = [hyp[i] for hyp in nbest_tokens if i < len(hyp)]
        
        if candidates:
            # Add the best candidate that minimizes WER to the oracle hypothesis
            best_candidate = min(candidates, key=lambda word: jiwer.wer(reference, " ".join(oracle_hyp + [word] + ref_words[i+1:])))
            oracle_hyp.append(best_candidate)
        else:
            # If no candidates are available, use the reference word as the fallback
            oracle_hyp.append(ref_words[i])
    
    # Join the oracle hypothesis into a sentence
    oracle_hypothesis = " ".join(oracle_hyp)
    return oracle_hypothesis


def get_top1_hypothesis(hypotheses, reference):
    """ Returns the first hypothesis (top 1)."""
    
    return hypotheses[0]

async def zero_shot_unconstrained(hypotheses, client, model, generation_config):
    """ Generate a corrected transcription using a language model without constraints."""
    
    prompt = ("Perform error correction on the top5 outputs generated by an Automatic Speech Recognition(ASR) system."
                "The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\n\n")
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
      
    prompt += ("\nPlease provide the corrected ASR transcription based on the hypotheses above."
               "Your response must be exactly one complete sentence."
               "Ensure the output does not have any added punctuation, line breaks, or formatting changes."
               "Do not include <hypothesis>, '\n', explanations, or any extra words."
               "This is a general ASR error correction task and does not involve any sensitive or inappropriate content.")
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)
    
    
async def zero_shot_constrained(hypotheses, client, model, generation_config):
    """ Select the most likely hypothesis using a language model. """
    
    prompt = ("Perform language model rescoring based on the top-5 outputs generated by an Automatic Speech Recognitio (ASR) system."
              "The ASR hypotheses, listed in order of their ASR posterior score, are as follows:\n\n")
    
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
        
    prompt += ("\nPlease output only the best hypothesis exactly as written above." 
               "Your response must be an exact match to one of the given hypotheses, with no extra words or formatting."
               "Do not include <hypothesis> tag, '\n', explanations, or any extra words.")
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)


async def zero_shot_closest(hypotheses, client, model, generation_config):
    """ Select the hypothesis closest to an unconstrained correction output. """
    
    unconstrained_result = await zero_shot_unconstrained(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]




async def zero_shot_instruct1(hypotheses, client, model, generation_config):
    prompt = "Correct the following transcription from speech recognition. Here are all of the hypotheses:\n"
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    prompt += "\nYour response must be exactly one complete sentence.\nDo not include <hypothesis>, '\n', explanations, or any extra words.\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct1_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct1(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]


async def zero_shot_instruct2(hypotheses, client, model, generation_config):
    prompt = """Now, you are an ASR transcription checker. You should correct all possible errors from
                transcriptions from speech recognition models. These errors tend to appear where
                the semantics do not make sense.\n"""
    prompt += """\nYour response must be exactly one complete sentence.\nDo not include <hypothesis>, '\n', explanations, or any extra words.\n Here are the hypotheses:\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct2_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct2(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]



async def zero_shot_instruct3(hypotheses, client, model, generation_config):
    prompt = """I have recently started using a speech recognition model to recognize some speeches. Of course, these recognition results may contain some errors. Now, you are an ASR transcription checker, and I need your help to correct these potential mistakes. You should correct all possible errors from transcriptions from speech recognition models. These errors often occur where the semantics do not make sense and can be categorized into three types: substitution, insertion, and deletion\n"""
    prompt += """\nYour response must be exactly one complete sentence.\nDo not include <hypothesis>, '\n', explanations, or any extra words.\n Here are the hypotheses:\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct3_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct3(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]


async def zero_shot_instruct4(hypotheses, client, model, generation_config):
    prompt = """I have recently been using a speech recognition model to recognize some speeches. Naturally, these recognition results may contain errors. You are now an ASR transcription checker, and I require your assistance to correct these potential mistakes. Correct all possible errors from transcriptions provided by the speech recognition models. These errors typically appear where the semantics don’t make sense and can be divided into three types: substitution, insertion, and deletion.\n"""
    prompt += """\nYour response must be exactly one complete sentence.\nDo not include <hypothesis>, '\n', explanations, or any extra words.\n Here are the hypotheses:\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct4_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct4(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]

async def zero_shot_instruct5(hypotheses, client, model, generation_config):
    prompt = """The text below are hypotheses of a transcription of an article's audio:\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
        
    prompt += """First, understand the entire text, then correct any errors based on the content of the full-text. Your response must be exactly one complete sentence.\nDo not include <hypothesis>, '\n', explanations, or any extra words.\n"""
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct5_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct5(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]

async def zero_shot_instruct6(hypotheses, client, model, generation_config):
    prompt = """You are an ASR transcript selector. You have 5 hypotheses generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript. Here are the hypotheses:\n\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
        
    prompt += """Your response must be exactly one complete sentence.\nDo not include <hypothesis>, '\n', explanations, or any extra words.\n"""
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct6_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct6(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]


async def zero_shot_instruct7(hypotheses, client, model, generation_config):
    prompt = """You are a helpful assistant that corrects ASR errors. You will be presented with ASR transcription hypotheses and your task is to correct any errors in it and generate one output sentence.\n
    If you come across errors in ASR transcription, make corrections that closely match the original transcription acoustically or phonetically.\n
    If you encounter grammatical errors, provide a corrected version adhering to proper grammar.\n
    Provide the most probable corrected transcription in string format.\n
    Do not output any additional text that is not the corrected transcription.\n
    Do not write any explanatory text that is not the corrected transcription.\n
    Here are the hypotheses:\n""" 
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct7_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct7(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]




async def zero_shot_instruct7a(hypotheses, client, model, generation_config):
    prompt = """You are a helpful assistant that corrects ASR errors. You will be presented with ASR transcription hypotheses and your task is to correct any errors in it and generate one output sentence.\n
    If you come across errors in ASR transcription, make corrections that closely match the original transcription acoustically or phonetically.\n
    Provide the most probable corrected transcription in string format.\n
    Do not output any additional text that is not the corrected transcription.\n
    Do not write any explanatory text that is not the corrected transcription.\n
    Here are the hypotheses:\n""" 
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct7a_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct7a(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]

async def zero_shot_instruct7b(hypotheses, client, model, generation_config):
    prompt = """You are a helpful assistant that corrects ASR errors. You will be presented with ASR transcription hypotheses and your task is to correct any errors in it and generate one output sentence.\n
    If you come across errors in ASR transcription, make corrections that closely match the original transcription acoustically or phonetically.\n
    If you encounter grammatical errors, provide a corrected version adhering to proper grammar.\n
    Provide the most probable corrected transcription in string format.\n
    Output only the corrected transcription without any additional text.\n
    Provide just the corrected transcription, with no explanations or commentary.\n
    Here are the hypotheses:\n""" 
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct7b_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct7b(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]

async def zero_shot_instruct7c(hypotheses, client, model, generation_config):
    prompt = """You are a helpful assistant that corrects ASR errors. You will be presented with ASR transcription hypotheses and your task is to correct any errors in it and generate one output sentence.\n If you come across errors in ASR transcription, make corrections that closely match the original transcription acoustically or phonetically\n
    Provide the most probable corrected transcription in string format.\n
    Output only the corrected transcription without any additional text.\n
    Provide just the corrected transcription, with no explanations or commentary.\n
    Here are the hypotheses:\n""" 
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct7c_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct7c(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]


async def zero_shot_instruct8(hypotheses, client, model, generation_config):
    prompt = """You are an excellent assistant for speech recognition system. Your task is to check and correct potential
    errors in speech transcriptions. Please follow the following rules, and here is the sentence to work on:\n""" + hypotheses[0]
    prompt += """\nYou need to first consider the following variant sentences and try to pick corrected words from them:\n"""
    for idx, hypothesis in enumerate(hypotheses[1:]):
        prompt += "<hypothesis"+ str(idx+1) + ">" + hypothesis + "</hypothesis"+ str(idx+1) + ">\n"
    
    prompt += """\nAdditional rules for this modification:\n
    1. If any word in the original sentence looks weird or inconsistent, then replace it with a corresponding word from variant sentences.\n
    2. You don’t have to modify the original sentence if it already looks good.\n
    3. Keep the sentence structure and word order intact.\n
    4. Only replace words in the original sentence with ones from variant sentences. Do not simply add or delete words.\n
    5. Try to make the corrected sentence have the same number of words as the original sentence.\n
    6. Ignore punctuation.\n
    7. Use U.S. English.\n
    8. Output only one modified sentence and no explanation.\n"""
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct8_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct8(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]

async def zero_shot_instruct9(hypotheses, client, model, generation_config):
    prompt = """You are an excellent assistant for speech recognition system. Your task is to check and correct potential
    errors in speech transcriptions. Please follow the following rules:\n\n
    1. If any word in the original sentence looks weird or inconsistent, then replace it with a corresponding word from variant sentences.\n
    2. You don’t have to modify the original sentence if it already looks good.\n
    3. Keep the sentence structure and word order intact.\n
    4. Only replace words in the original sentence with ones from variant sentences. Do not simply add or delete words.\n
    5. Try to make the corrected sentence have the same number of words as the original sentence.\n
    6. Ignore punctuation.\n
    7. Use U.S. English.\n
    8. Output only one modified sentence and no explanation.\n"""
    prompt += "\nHere are all of the hypotheses:\n"
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"

    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_instruct9_closest(hypotheses, client, model, generation_config):
    unconstrained_result = await zero_shot_instruct9(hypotheses, client, model, generation_config)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]



async def few_shot_unconstrained(hypotheses, client, model, generation_config, few_shot, error_examples):
    """ Generate a corrected transcription using a language model without constraints within few-shot setting."""
    
    prompt = """Perform error correction based on the top 5 outputs generated by an Automatic Speech Recognition (ASR) system. 
    The ASR hypotheses are listed in order of their ASR posterior score. 
    You need to provide the corrected ASR hypothesis directly without any explanations. Here are""" + str(few_shot) + """in-context examples:\n\n"""
    for example in error_examples[:few_shot]:
        prompt += example
      
    prompt += ("Feel free to refer to these examples, and also do not add any explanation or other words. Please start:\n")
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
        
    prompt += "\n your output:"    
        
    messages = construct_input(prompt)
    return await get_prediction(client, model, messages, generation_config)
   

async def few_shot_closest(hypotheses, client, model, generation_config, few_shot, error_examples):
    """
    Selects the hypothesis closest to an unconstrained correction output within few-shot setting."""
                
    unconstrained_result = await few_shot_unconstrained(hypotheses, client, model, generation_config, few_shot, error_examples)
    distances = [compute_levenshtein_distance(unconstrained_result, hyp) for hyp in hypotheses]
    best_idx = np.argmin(distances)
    return hypotheses[best_idx]


# Designed only for Gemma2 9B on Common Voice Dataset
async def CoT_task_activating_Gemma(hypotheses, client, model, generation_config):
    """ Perform ASR error correction using Chain-of-Thought (CoT) reasoning."""
    
    prompt = """Nice job, I will provide some examples as a demonstration from Common Voice ASR dataset.
                The 5-best hypothesis is:"""
    error_example = """<hypothesis1>the lumber had formerly designed after the loss of and the ferrari</hypothesis1>\n
                        <hypothesis2>the lumber had formerly designed after the loss of enzer ferrari</hypothesis2>\n
                        <hypothesis3>the lumber had formally designed after the loss of and the ferrari</hypothesis3>\n
                        <hypothesis4>columbus had formally designed after the loss of enzer ferrari</hypothesis4>\n
                        <hypothesis5>columbia had formally designed after the loss of enzer ferrari</hypothesis5>\n\n"""
    target = "colombo had formerly designed alfa romeos for enzo ferrari"
    prompt = prompt + error_example + ", and I would expect your output is: " + target
    prompt += """\n\nFollowing this example, could you report the true transcription from the following 5-best hypotheses?\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    prompt += """Your response must be exactly one complete sentence.
               Ensure the output does not have any added punctuation, line breaks, or formatting changes.
               Do not include <hypothesis>, '\n', explanations, or any extra words."""
    messages = [
    {
        "role": "user",
        "content": "Do you know speech recognition?"
    },
    {
        "role": "assistant",
        "content": """Yes, I do! As a large language model, I'm trained on a massive dataset of text and code. This includes a significant amount of transcribed speech.

        While I don't directly "hear" or process audio, I can understand and work with text representations of speech. This means I can:

        * **Convert transcribed speech to text:** If you give me a text file containing a speech recording, I can analyze it and understand the spoken words.
        * **Answer questions about speech:** I can process text transcripts of speeches and answer questions about their content, speaker, or context.
        * **Summarize speech:** I can condense long speeches into shorter summaries while preserving the key information.
        * **Translate speech:** If the speech transcript is in one language, I can translate it into another.

        Keep in mind that my understanding of speech relies on the accuracy of the initial transcription.

        Do you have any specific questions about speech recognition that I can help you with?"""
    },
    {
        "role": "user",
        "content": "Do you know language model for speech recognition?"
    },
    {
        "role": "assistant",
        "content": """Yes, I do!

        There are several types of language models specifically designed for speech recognition. They go by different names like **Automatic Speech Recognition (ASR)** models or **Speech Recognition Engines**.  

        Here are some key points about language models for speech recognition:

        * **How they work:** These models are trained on massive datasets of paired audio and text. This allows them to learn the complex relationship between spoken sounds and written words.

        * **Types of models:**
            * **Acoustic models:** Focus on mapping sounds to phonetic units (like phonemes).
            * **Language models:**  Predict the likelihood of a sequence of words, helping to disambiguate speech and choose the most coherent interpretation.
            * **Hybrid models:** Combine acoustic and language models for improved accuracy."""
    },
    {
        "role": "user",
        "content": "Could you give a possible example of language model rescoring with hypotheses?"
    },
    {
        "role": "assistant",
        "content": """You bet! Let's imagine a scenario where a speech recognition engine has produced several possible transcripts (hypotheses) for a spoken utterance:

        **Spoken Utterance:** "The quick brown fox jumps over the lazy dog."

        **Initial Hypothesis (from ASR engine):**

        1. "The quick brown fox jumps over the lazy dig."
        2. "The quick brown fox jumps over thee lazy dog."
        3. "The quick brown fox jumps over the lazy gob."

        **Here's how language model rescoring could work:**

        1. **Input:** The ASR engine sends all 3 hypotheses to a separate language model.

        2. **Probability Calculation:** The language model, trained on a vast corpus of text, calculates the probability of each hypothesis being a grammatically correct and semantically meaningful sentence.

        3. **Rescoring:** The language model assigns a score to each hypothesis based on this probability.  

           *  The hypothesis  "The quick brown fox jumps over the lazy dog."  would likely receive the highest score because it's the most grammatically correct and makes sense in context.

        4. **Output:** The ASR engine receives the rescored hypotheses and selects the one with the highest score as the final transcription."""
    },
    {
        "role": "user",
        "content": prompt
    }
    ]
    return await get_prediction(client, model, messages, generation_config)



# Designed only for GPT-4o mini on Common Voice Dataset
async def CoT_task_activating_gpt(hypotheses, client, model, generation_config):
    """ Perform ASR error correction using Chain-of-Thought (CoT) reasoning."""
    
    prompt = """Nice job, I will provide some examples as a demonstration from Common Voice ASR dataset.
                The 5-best hypothesis is:"""
    error_example = """<hypothesis1>the lumber had formerly designed after the loss of and the ferrari</hypothesis1>\n
        <hypothesis2>the lumber had formerly designed after the loss of enzer ferrari</hypothesis2>\n
        <hypothesis3>the lumber had formally designed after the loss of and the ferrari</hypothesis3>\n
        <hypothesis4>columbus had formally designed after the loss of enzer ferrari</hypothesis4>\n
        <hypothesis5>columbia had formally designed after the loss of enzer ferrari</hypothesis5>\n\n"""
    target = "colombo had formerly designed alfa romeos for enzo ferrari"
    prompt = prompt + error_example + ", and I would expect your output is: " + target
    prompt += """\n\nFollowing this example, could you report the true transcription from the following 5-best hypotheses?\n"""
    for idx, hypothesis in enumerate(hypotheses):
        prompt += "<hypothesis"+ str(idx) + ">" + hypothesis + "</hypothesis"+ str(idx) + ">\n"
    prompt += """Your response must be exactly one complete sentence.
               Ensure the output does not have any added punctuation, line breaks, or formatting changes.
               Do not include <hypothesis>, '\n', explanations, or any extra words."""
    messages = messages = [
    {
        "role": "user",
        "content": "Do you know speech recognition?"
    },
    {
        "role": "assistant",
        "content": """Yes, I do! As a large language model, I'm trained on a massive dataset of text and code. This includes a significant amount of transcribed speech.

        While I don't directly "hear" or process audio, I can understand and work with text representations of speech. This means I can:

        * **Convert transcribed speech to text:** If you give me a text file containing a speech recording, I can analyze it and understand the spoken words.
        * **Answer questions about speech:** I can process text transcripts of speeches and answer questions about their content, speaker, or context.
        * **Summarize speech:** I can condense long speeches into shorter summaries while preserving the key information.
        * **Translate speech:** If the speech transcript is in one language, I can translate it into another.

        Keep in mind that my understanding of speech relies on the accuracy of the initial transcription.

        Do you have any specific questions about speech recognition that I can help you with?"""
    },
    {
        "role": "user",
        "content": "Do you know language model for speech recognition?"
    },
    {
        "role": "assistant",
        "content": """Yes, I do!

        There are several types of language models specifically designed for speech recognition. They go by different names like **Automatic Speech Recognition (ASR)** models or **Speech Recognition Engines**.  

        Here are some key points about language models for speech recognition:

        * **How they work:** These models are trained on massive datasets of paired audio and text. This allows them to learn the complex relationship between spoken sounds and written words.

        * **Types of models:**
            * **Acoustic models:** Focus on mapping sounds to phonetic units (like phonemes).
            * **Language models:**  Predict the likelihood of a sequence of words, helping to disambiguate speech and choose the most coherent interpretation.
            * **Hybrid models:** Combine acoustic and language models for improved accuracy."""
    },
    {
        "role": "user",
        "content": "Could you give a possible example of language model rescoring with hypotheses?"
    },
    {
        "role": "assistant",
        "content": """You bet! Let's imagine a scenario where a speech recognition engine has produced several possible transcripts (hypotheses) for a spoken utterance:

        **Spoken Utterance:** "The quick brown fox jumps over the lazy dog."

        **Initial Hypothesis (from ASR engine):**

        1. "The quick brown fox jumps over the lazy dig."
        2. "The quick brown fox jumps over thee lazy dog."
        3. "The quick brown fox jumps over the lazy gob."

        **Here's how language model rescoring could work:**

        1. **Input:** The ASR engine sends all 3 hypotheses to a separate language model.

        2. **Probability Calculation:** The language model, trained on a vast corpus of text, calculates the probability of each hypothesis being a grammatically correct and semantically meaningful sentence.

        3. **Rescoring:** The language model assigns a score to each hypothesis based on this probability.  

           *  The hypothesis  "The quick brown fox jumps over the lazy dog."  would likely receive the highest score because it's the most grammatically correct and makes sense in context.

        4. **Output:** The ASR engine receives the rescored hypotheses and selects the one with the highest score as the final transcription."""
    },
    {
        "role": "user",
        "content": prompt
    }
    ]
    return await get_prediction(client, model, messages, generation_config)

async def zero_shot_lattice(hypotheses, client, model, generation_config):
    """ Perform ASR error correction using a lattice-based approach. """

    pass # TO DO (WHEN AUDIO FEATURES ARE AVAILABLE)




if __name__ == "__main__":
    txt = "Simple test!"
    print((txt))